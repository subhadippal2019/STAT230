\documentclass[compress]{beamer}
\mode<presentation>
\setbeamercovered{transparent}
\usetheme{Warsaw}
%\useoutertheme{smoothtree}
\usepackage{multirow}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{xmpmulti}
\usepackage{multicol}
\usepackage{colortbl}

%\setbeamersize{text margin left=.25 in,text margin right=.25 in}
\setbeamersize{text margin left=.15 in,text margin right=.15 in}
\usepackage[authoryear]{natbib}


\usepackage{epstopdf}
\usepackage{xcolor}
\usepackage{latexcolors}
%\usepackage[dvipsnames]{xcolor}
\definecolor{antiquebrass}{rgb}{0.8, 0.58, 0.46}
\definecolor{babyblueeyes}{rgb}{0.63, 0.79, 0.95}
\definecolor{babyblue}{rgb}{0.54, 0.81, 0.94}
\definecolor{bistre}{rgb}{0.24, 0.17, 0.12}
\definecolor{brightlavender}{rgb}{0.75, 0.58, 0.89}
\definecolor{bulgarianrose}{rgb}{0.28, 0.02, 0.03}
\definecolor{slateblue}{rgb}{0.56, 0.74, 0.56}
\definecolor{cordovan}{rgb}{0.54, 0.25, 0.27}
\definecolor{darkbyzantium}{rgb}{0.36, 0.22, 0.33}

\setbeamercolor{structure}{fg=bittersweet!70, bg= black!60}







\usepackage{tikz}
\usetikzlibrary{shadows,calc}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes.symbols}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
%%%%%%%%% shaddow image %%%%%
% some parameters for customization
\def\shadowshift{3pt,-3pt}
\def\shadowradius{6pt}
\colorlet{innercolor}{black!60}
\colorlet{outercolor}{gray!05}
% this draws a shadow under a rectangle node
\newcommand\drawshadow[1]{
\begin{pgfonlayer}{shadow}
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.south west)+(\shadowshift)+(\shadowradius/2,\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.north east)+(\shadowshift)+(-\shadowradius/2,-\shadowradius/2)$) circle (\shadowradius);
    \shade[top color=innercolor,bottom color=outercolor] ($(#1.south west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) rectangle ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$);
    \shade[left color=innercolor,right color=outercolor] ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$);
    \shade[bottom color=innercolor,top color=outercolor] ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$);
    \shade[outercolor,right color=innercolor,left color=outercolor] ($(#1.south west)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$);
    \shade[outercolor,right color=innercolor,left color=innercolor] ($(#1.north west)+(-\shadowradius/12,\shadowradius/12)$) rectangle ($(#1.south east)+(\shadowradius/12,-\shadowradius/12)$);%Frame
    \filldraw ($(#1.south west)+(\shadowshift)+(\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)-(\shadowradius/2,\shadowradius/2)$);
\end{pgfonlayer}
}
% create a shadow layer, so that we don't need to worry about overdrawing other things
\pgfdeclarelayer{shadow} 
\pgfsetlayers{shadow,main}
% Define image shadow command
\newcommand\shadowimage[2][]{%
\begin{tikzpicture}
\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[#1]{#2}};
\drawshadow{image}
\end{tikzpicture}}
\usepackage{calligra}

\DeclareMathOperator*{\argmax}{Arg\,max}
\DeclareMathOperator*{\argmin}{Arg\,min}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert }
\newcommand{\bbetaHat}{ \widehat{\bbeta}}
\newcommand{\bbetaLSE}{ \widehat{\bbeta}_{_{\text{LSE}}}}
\newcommand{\bbetaMLE}{ \widehat{\bbeta}_{_{\text{MLE}}}}
\newcommand{\sqBullet}[1]{  {\tiny \tiny \tiny \qBoxCol{#1!60}{ }} }
%***************
%\newtheorem{thm}{Theorem}
\input{../LatexSupportFiles/definition_include}
\input{../LatexSupportFiles/BoxDef}
\input{../LatexSupportFiles/MatrixDef}











%\title{  STAT 320: Principles of Probability\\ {\color{black}   \HLTW{\text{Unit 5} (\HLTY{\text{PART:A}})}:\\
%Introduction to  Random Variables \& Discrete Random Variables}}



\title{  STAT 320: Principles of Probability\\ {\color{black}    \HLTW{\text{Unit 5} (\HLTY{\text{PART:A}})}\\ 
\qBrd[3in]{rose!30}{Introduction to  Random Variables \& Discrete Random Variables}}}

\author[UAEU]
{United Arab Emirates University}
\institute[UAEU] % (optional, but mostly needed)
{
  \inst{Department of Statistics}%
  %Indian Institute of Management,  Udaipur\\
  \vspace{0.1in}

  
}

\date{}


\newcommand{\Xnew}{ \HLTEQ[orange]{X_{_{\text{i}}}} }
\newcommand{\Ynew}{ \HLTEQ[orange]{Y_{_{\text{i}}}} }

%\date{\today}

\AtBeginSection[]
{
  \begin{frame}{Inhalt}
 % \begin{multicols}{1}
	\frametitle{Outline}
    \tableofcontents[currentsection]
  %  \end{multicols}
  \end{frame}
}

\begin{document}
\maketitle

%\begin{frame}{Outline}
%%\begin{multicols}{}
%  \tableofcontents
%%\end{multicols}
%\end{frame}

%\section{Introduction to DSBA 2023}
%
%
%\begin{frame}
%\qBoxCol{blue!30}{
%\begin{center} Course  Website \end{center}
%\qbx[4.2in]{teal!40}{\sqBullet{teal} \color{blue} $ \href{https://sites.google.com/iimu.ac.in/dsba2023e/home}{https://sites.google.com/iimu.ac.in/dsba2023e/home}$
%}\\
%\qbx[3.0in]{green!40}{ \sqBullet{green} Regular Announcements.
%}\\
%\qbx[3.0in]{olive!40}{\sqBullet{olive}  Slides and other materials.
%}
%}
%
%\pause
%\qBoxCol{blue!30}{
%\sqBullet{blue}
%You can contact the instructor at {\it subhadip.pal@iimu.ac.in} and schedule for office hours.  
%}
%\pause
%\qBoxCol{olive!30}{
%\sqBullet{olive}
%Mr. Praveen Kumar has been assigned as Teaching Assistant (TA) for this course.  His email I'd is:  {\it praveen.kumar@iimu.ac. }
%}
%
%
%\end{frame}
%


%
%\begin{frame}{Course Outline}
%\hspace{-.1in}\qBoxCol{blue!35}{
%% Please add the following required packages to your document preamble:
%% \usepackage{booktabs}
%\begin{table}[]
%\begin{tabular}{@{}lll@{}}
%\toprule
%         & Topics                                                & Dataset or Case                                    \\ \midrule \midrule
%\rowcolor{blue!20}     \multicolumn{1}{|l|}{1-2}   & \multicolumn{1}{l|}{Overview of Data Science}        & \multicolumn{1}{l|}{Household Data}                \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{3-5}   & \multicolumn{1}{l|}{Data Visualization}              & \multicolumn{1}{l|}{Global Super Store }       \\ \midrule
%\rowcolor{blue!20} 
%\multicolumn{1}{|l|}{6}     & \multicolumn{1}{l|}{Introduction to R/ JMP}          & \multicolumn{1}{l|}{}                              \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{7}     & \multicolumn{1}{l|}{Regression Analysis}             & \multicolumn{1}{l|}{Display \& Liquor Sales} \\ \midrule
%\rowcolor{blue!20} 
%\multicolumn{1}{|l|}{8}     & \multicolumn{1}{l|}{Multiple Regression}             & \multicolumn{1}{l|}{}                              \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{9}     & \multicolumn{1}{l|}{Dealing with Nominal Covariates} & \multicolumn{1}{l|}{Gender Divide}                 \\ \midrule
%\rowcolor{blue!20} 
%\multicolumn{1}{|l|}{10}    & \multicolumn{1}{l|}{Regression Diagonistics}         & \multicolumn{1}{l|}{}                              \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{11-12} & \multicolumn{1}{l|}{Project Presentations}            &\multicolumn{1}{l|}{}          \\\midrule \bottomrule
%\end{tabular}
%\end{table}
%}
%\end{frame}


%\begin{frame}{Case Study }
%\qBoxCol{teal!40}{\vspace{1in}\begin{center}\sqBullet{teal} \Large Case: Liquor sales and display space \end{center}
%\vspace{1in}
%}\\
%\end{frame}







\section{Random Variables   }
\TransitionFrame[bittersweet]{\Large Random Variables  }

\begin{frame}\frametitle{}
\vspace{-.05in}
\qbx[4.5in]{applegreen!40}{
 In majority of the scenarios, we are often interested some specific numeric attributes of the data rather then overall outcome that often may be recorded in terms of symbols.
}
\vspace{.01in}
\qbx[4.5in]{babyblue!60}{
Events of major interests to a professional dealing with data are mostly numerical in nature.
}
\vspace{-.15in}\\
\begin{itemize}
\qBrd[4.2in]{bazaar!60}{\small
\item[\sqBullet{bazaar}] A business person might be interested in aggregate sales in the last few years and wan to predict the amount for the next year.  
}
\vspace{.05in}\\
\qBrd[4.2in]{blush!60}{\small
\item[\sqBullet{blush}]  An associate in an  insurance company may be interested in the funds that should be made available/reserved to compensate the losses of its customers. 
}
\vspace{.05in}\\
\qBrd[4.2in]{amber!60}{\small
\item[\sqBullet{amber}]  A marketing agent may be interested in determining the additional orders that may take place due to a specific sales-promotion that he plans to launch. 
}
\end{itemize}	





\end{frame}

\begin{frame}{Random Variables}
\begin{itemize}
\qBrd[4.2in]{bazaar!60}{
\item[\sqBullet{bazaar}]  A television manufacturer might be interested in lifetime of a newly designed led screen that they have designed.  
}
\vspace{.1in}\\
\qBrd[4.2in]{blush!60}{
\item[\sqBullet{blush}] In a much simple statistical experiment, such as in tossing dices, we may be interested in the number of claims that that are likely to be filed within a specific time period. . 
}
\end{itemize}	


\qbx[4.5in]{applegreen!40}{
 These quantities of interest, or, more formally, these real-valued functions defined on the sample space, are known as random variables.}
\vspace{-.1in}
\define{Random Variable }{
A random variable is a function from a sample space $\SampleS$ into the real numbers.}
\end{frame}


\begin{frame}{Example: Random Variable }

\begin{table}
\centering
\begin{tabular}{|l|l|} 
\hline
 Experiment &  Random Variable \\ 
 \hline
\hline
 Toss two dice & X= sum of the numbers   \\ 
 \hline
Toss a coin 25 times  &  X number of heads in 25 tosses \\ 
\hline
 Apply different amounts of\\ fertilizer to corn plants &  X = yield/acre \\ 
\hline
\end{tabular}
\end{table}


\qBox{{\bf Notation:} Random variables will always be denoted with uppercase letters
and the realized values of the variable (or its range) will be denoted by the corresponding
lowercase letters. Thus, the random variable X can take the value x.}

%\qBox{Probabilities of a random variable is }
\end{frame}




\begin{frame}{Support/Range of a Random Variable }

\define{Support/Range of a Random Variable}{
The set containing the all possible values of a random variable is called its {\it \bf support} or {\it  \bf range}.
}
\vspace{.2in}
\qBox{{\bf Notation:} We will use the notation $\support[X]$ ( or simply $\support$ if there is no ambiguity)  to denote the support of a random variable X.}

\qBox{{\bf Example}: Consider  the experiment of tossing
a fair coin 3 times from. Define the random variable X to be the
number of heads obtained in the 3 tosses.  The Support of the random variable is $\HLTEQ[yellow]{\support[X]=\{0, 1, 2, 3\}}$}
%
%\qBox{{\bf Example:}
%A car insurance company in interested in the number of car accidents that may happen in a specific week of the year. 
%}
\end{frame}







\begin{frame}\frametitle{Example}
\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{\small
\Exmpl{amethyst}{}  Suppose that our experiment consists of tossing 3 fair coins. If we let $Y$ denote the number of heads that appear, then $Y$ is a random variable taking on one of the values $ 0, 1, 2,$ and $3$ with respective probabilities.
\begin{itemize}
\item[] $\pmf_{_Y}(0)= P(Y=0 )=$
\item[] $\pmf_{_Y}(1)= P(Y=1 )=$
\item[] $\pmf_{_Y}(2)= P(Y=2 )=$
\item[] $\pmf_{_Y}(3)= P(Y=3 )=$
\end{itemize}
}\\
%\pause
\vspace{1.6in}
{\tiny 
}
\end{frame}






\section{Discrete Random Variables  }
\TransitionFrame[bittersweet]{\Large Discrete Random Variables \& its Probability Mass Function \HLTW{\text{(pmf)}}  }

\begin{frame}
\define{Discrete Random Variables}{A random variable that can take on at most a countable number of possible values is said to be discrete.  {\it That is, if $ \support$,  the support of a random variable is finite or countable infinite then the corresponding random variable is discrete. }}
\vspace{.05in}

\qBrd[4.6in]{babyblue!70}{
\qBrd[2.35in]{babyblue!40}{Probability Mass Function (pmf)}
For a discrete random variable $X$, we define the probability
mass function (pmf) $\pmf_{_X}(x)$ of $X$ by 
$$\HLTW{\HLTY{\pmf_{_X}(x)}= P(X=x) }\text{ for all } \HLTW{ x\in \support[X]}$$
}

\vspace{1.5in}

\end{frame}





\begin{frame}
\qBrd[4.5in]{applegreen!30}{
Let $X$ be a discrete random variable with probability mass function $\HLTY{\pmf(x)}$ defined on the support $\support$. 
Let $\HLTW{\HLTEQ[amethyst!40]{A}\subset \support}$ be an event, then 
\begin{center}
\qBrd[3.5in]{applegreen!50}{$$ \HLTW{   P\left(\HLTEQ[amethyst!40]{A}\right)}:=  P(X\in A)=  \HLTW{  \displaystyle \sum_{\left\{x\in \HLTEQ[amethyst!40]{A}\right\}} \HLTY{\pmf(x)}}.$$} 
\vspace{.1in}
\end{center}
}

\end{frame}


\begin{frame}{}
\vspace{-.1in}
{\small The pmf of the random variable representing the sum when two dice are rolled can be represented in multiple ways.}
\qBrd[4.7in]{babyblue!40}{
\begin{center}
{\small As a Tabular Format: }
\qBrd[4.3in]{babyblue!70}{
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
$x$    & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\ \hline
$\pmf_{_X}(x)$ & $\frac{1}{36}$  &$ \frac{2}{36}$  & $\frac{3}{36}$  &$ \frac{4}{36} $  &    $\frac{5}{36} $ &$ \frac{6}{36}  $ &  $\frac{5}{36} $ & $ \frac{4}{36}$   &  $ \frac{3}{36}$  & $ \frac{2}{36}$ &$ \frac{1}{36}$  \\ \hline
\end{tabular}
}
\end{center}
}\\
\vspace{.1in}
\begin{minipage}{.49\textwidth} %
\qBrd[2.3in]{blue!30}{
\begin{center}
{\small As a Plot/Graph:} \\
\includegraphics[scale=.25]{figs/TwoFairDice_pmf_Chart.png}
\vspace{.1in}
\end{center}}
\end{minipage} %
\hspace{0.05in}
\begin{minipage}{.47\textwidth} %
\qBrd[2.3in]{antiquefuchsia!40}{
\begin{center}
{\small As a Function} \\
\qBrd[2.1in]{amethyst!70}{
$$\pmf_{_X}(x)= \begin{cases}
\frac{x-1}{36} & \text{ if }  2\leq x\leq 7, \\
\frac{13-x}{36} & \text{ if }  8\leq x\leq 12 \\
 \end{cases}$$
 }
 \vspace{1in}
  \end{center}
  }
\end{minipage}
 

\end{frame}


\begin{frame}
\qBrd[4.6in]{amber!40}{
\begin{center}
\qBrd[2.6in]{amber!70}{\Large Characterization of a pmf }\\
\end{center}
Let $\HLTW{\pmf(x)}$ is {\bf probability mass function } of a discrete random variable on the support $\support$,  {\bf if and only if} it satisfies the following conditions:
\begin{enumerate}
\qBrd[3in]{babyblue!40}{\item {\it Positivity: }$ \HLTW{ \displaystyle  \pmf(x)> 0}$ for all $\HLTW{ x\in \support}$}
\qBrd[3.2in]{applegreen!40}{\item {\it Total Probability:} $\HLTW{   \displaystyle  \sum_{\{x\in \support\}} \pmf(x)=1}$. }
\end{enumerate}
}

\end{frame}

\begin{frame}
\qBrd[4.6in]{airforceblue!30}{
\Exmpl{babyblue}{} \small
Suppose a random variable $X$ has the following support $\support[X]= \{1, 2,3, 4, 5\}$. 
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline\hline
x    & 1    & 2    & 3 & 4& 5    \\ \hline\hline
$\pmf_{_X}(x)$ &$ \frac{1}{5}$ & $ \frac{2}{5}$  &$ \frac{1}{10}$  & \HLTY{ c} &$ \frac{1}{10}$   \\ \hline\hline
\end{tabular}
\end{center}
\begin{enumerate}
\item What is the value of \HLTY{ c} so that $\pmf_{_X}(x)$ become a valid probability mass function on the support $\support[X]$?
\item What is the probability that $X\in A$, where the event $A=\{2, 4\}$.
\end{enumerate}
}
\vspace{2in}
\end{frame}




\begin{frame}
\qbx[4.5in]{apricot!50}{
\Exmpl{amber}{} \small
Suppose a random variable $X$ has the following support $\support[X]= \{0,1, 2,3, \ldots \}$. 
The corresponding pmf is 
$$ \pmf_{_X}(x):= \frac{\HLTY{c}}{5^x} \text{ for } x= 0, 1, 2, 3, \ldots  ,  $$
where \HLTY{c} is an appropriately chosen constant. 
\begin{enumerate}
\item What is the value of \HLTY{ c} so that $\pmf_{_X}(x)$ become a valid probability mass function on the support $\support[X]$?
\item What is the probability that $X\in A$, where the event $A=\{1, 3, 5\}$.
\item What is the probability that $X\leq 3$?
\item What is the probability that $X>3$?
\end{enumerate}
}
\vspace{2in}
\end{frame}



\begin{frame}\frametitle{Example}
\vspace{-.1in}
\qbx[4.5in]{olive!40}{
\Exmpl{amber}{}  A system consists of 2 components connected in parallel, then at least one must work correctly for the system to work correctly.  Each component operates correctly with probability 0.8 and independent of the other.  Let X be the number of components that work correctly. Find the probability distribution of X. 
}\\
\pause
\vspace{.6in}
{\tiny 
{\bf Solution: } $X$ can take on only three possible values; $0, 1,$ or $2$. Let $E_i$ denote the event that component $i$ works correctly. Then $P(E_i) = 0.8$. Thus, we have
\begin{itemize}
\item $\pmf_{_X}(0)= P(\Not{E_1} \cap \Not{E_2})= P(\Not{E_1}) P( \Not{E_2})= (0.2)(0.2)= 0.04$.
\item $\pmf_{_X}(1)= P(\Not{E_1} \cap {E_2})+ P({E_1} \cap \Not{E_2})=(0.2)(0.8)+ (0.8)(0.2)=0 .32$.
\item  $\pmf_{_X}(2)= P({E_1} \cap {E_2})= P({E_1}) P( {E_2})= (0.8)(0.8)= 0.64$.
\end{itemize}
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline\hline
x    & 0    & 1    & 2    \\ \hline
$\pmf_{_X}(x)$ & 0.04 & 0.32 & 0.64 \\ \hline\hline
\end{tabular}
\end{center}
}
\end{frame}





\section{Cumulative Distribution Function \HLTW{\text{(CDF)}} of a discrete Random Variable}

\TransitionFrame[antiquefuchsia]{\Large Cumulative Distribution Function \HLTW{\text{(CDF)}} of a discrete Random Variable }
\begin{frame}\frametitle{ cumulative distribution function}
\define{cumulative distribution function}{
Let $X$ be a discrete random variable on the support $\support[X]$ with the corresponding probability mass function
$$\HLTEQ[teal!50]{\HLTW{ \displaystyle P(X=x)= \pmf_{_X}(x)} \text{ for } \HLTW{x\in \support[X]}.}$$
Then for any $a\in \R$, the cumulative distribution function (cdf), denoted by $\cdf_{_X}(\cdot)$ is the following quantity
$$\DBX{ \displaystyle \cdf_{_X}(\HLTY{a})= P(X\leq \HLTY{a})= \sum_{ \HLTEQ[teal!40]{ \{\HLTW{x}\leq \HLTY{a}: \HLTW{x}\in \support[X]\}}}\pmf_{_X}(\HLTW{x})}$$	
}
\end{frame}


\begin{frame}
\qBrd[4.5in]{amethyst!40}{\sqBullet{amethyst}
A {\bf pmf} of a discrete random variable is only positive/ revelant on the support of the random variable $\support$, However the {\bf CDF} is defined for any real number.  
}
\end{frame}


\begin{frame}\frametitle{Example}
\vspace{-.1in}
\qbx[4.5in]{amethyst!50}{
\Exmpl{amethyst}{} \small  If X be a discrete random variable on the support $\support[X]=\{1, 2, 3, 4\}$ with the corresponding  pmf specified as  
$\pmf_{_X}(1)= \frac{1}{4}, \pmf_{_X}(2)= \frac{1}{2}, \pmf_{_X}(3)= \frac{1}{8}$, and $\pmf_{_X}(4)= \frac{1}{8}$. Calculate the CDF function of $X$. 
}\\
\vspace{.3in}
\pause
\begin{minipage}{.48\textwidth} %
{\tiny 
{\bf Solution: } 
\qBrd[2.1in]{babyblue!40}{
\begin{eqnarray*}
\cdf_{_X}\left(\HLTY{a}\right)= 
\begin{cases}
                        0 &  \text{if  $\HLTY{a}<1$} \vspace{.1in}\\
                        \frac{1}{4} &  \text{if  $1\leq \HLTY{a} <2$}\vspace{.1in}\\
                        \frac{3}{4} &  \text{if  $2\leq \HLTY{a} <3$}\vspace{.1in}\\
                        \frac{7}{8} &  \text{if  $3\leq \HLTY{a} <4$}\vspace{.1in}\\
                        \frac{7}{8} &  \text{if  $4\leq \HLTY{a} $}
                    \end{cases}
                    \end{eqnarray*}
                    }
}
\end{minipage} %
\begin{minipage}{.48\textwidth} %
%\qBrd[2.3in]{babyblue!40}{
\begin{center}
{\tiny Graph of CDF of $X$}
\includegraphics[scale=.2]{figs/BlankGraph.png} \\
\end{center}
%}
\end{minipage}


\vspace{.1in}
\end{frame}





\begin{frame}\frametitle{}
\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
Let the pmf of a discrete random varibale $X$ is given as 
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline\hline
x    & 0    & 1    & 2    \\ \hline
$\pmf_{_X}(x)$ & 0.04 & 0.32 & 0.64 \\ \hline\hline
\end{tabular}
\end{center}
Find the corresponding CDF. 
}\\
\pause
\vspace{.1in}
\begin{minipage}{.48\textwidth} %
{\tiny 
{\bf Solution: } 
\qBrd[2.1in]{babyblue!40}{
\begin{eqnarray*}
\cdf_{_X}\left(\HLTY{a}\right)= 
\begin{cases}
                        0 &  \text{if  $\HLTY{a}<0$} \vspace{.1in}\\
                       0.04&  \text{if  $0\leq \HLTY{a}<1$}\vspace{.1in}\\
                       0.36 &  \text{if  $1\leq \HLTY{a} <2$}\vspace{.1in}\\
                       1&  \text{if  $2\leq \HLTY{a} $}
                    \end{cases}
                    \end{eqnarray*}
                    }
}
\end{minipage} %
\begin{minipage}{.48\textwidth} %
%\qBrd[2.3in]{babyblue!40}{
\begin{center}
{\tiny Graph of CDF of $X$}
\includegraphics[scale=.2]{figs/BlankGraph.png} \\
\end{center}
%}
\end{minipage}


\vspace{.1in}
\end{frame}



\section{Expected Value and Variance of a Discrete Random Variable  }
\TransitionFrame[antiquefuchsia]{\Large \HLTW{\text{Expected Value}} \& \HLTW{\text{Variance}}  of a Discrete Random Variable }

\begin{frame}\frametitle{The {\bf ``Expected Value''} or {\bf ``Mean''} of a Discrete Random Variable}
\define{\tiny The {\bf ``Expected Value''} or {\bf ``Mean''} of a Discrete Random Variable}{
If $X$ is a random variable with pmf $\HLTY{\pmf_{_X}(x)} $ on the support $\support[X]$ , then the expected value (the mean) of $X$ denoted by $E(X)$ ( or $\mu_{_X}$)  is given by
$$\DBX{ \displaystyle E\left(\HLTW{X}\right)= \sum_{\left\{ \HLTEQ[teal!40]{\HLTW{x}\in \support[X]} \right\}} \HLTW{x}  \; \HLTY{\pmf_{_X}(\HLTW{x})},}$$
assuming the  above summation/series exists /well-defined. 
}
\end{frame}



\begin{frame}
\define{\tiny The  Expected Value of a Function of a Discrete Random Variable}{Let the random variable $X$ has the probability mass funciton $\HLTY{\pmf_{_X}(x)}$ for all $\HLTEQ[teal!40]{x\in \support[X]}$, the support of X.  Let $\HLTW{{h}(x)}$ be any$^{*}$ function,  then the expected value of $ h(X)$ is defined as 
$$\DBX{ \displaystyle  E\left(\HLTW{h(X)} \right)= \sum_{\left\{\HLTEQ[teal!40]{\HLTW{x}\in \support[X] }\right\}} \HLTW{h(x)}  \; \HLTY{\pmf_{_X}(\HLTW{x})},}$$
assuming the  above summation/series exists /well-defined. 
}
\end{frame}





\begin{frame}\frametitle{Variance }

\qBrd[4.6in]{applegreen!35}{  \qBrd[.65in]{applegreen!60}{Variance }The variance of $X$, denoted by $\text{Var}(X)$ is deifined as 
$$\DBX{ \displaystyle \text{Var}(X):= \HLTW{E \left(X-\HLTY{ \mu_{_X}} \right)^2}},$$
\vspace{-.05in}
where $\HLTY{ \mu_{_X}}= E(X)$, the mean of the random variable. 
}
\vspace{-.1in}

\define{Variance}{ The variance of $X$, denoted by $\text{Var}(X)$ is deifined as 
$$\DBX{ \displaystyle \text{Var}(X):=\HLTW{ E(X^2)- \left(\HLTY{E(X)} \right)^2}}$$\\
\vspace{-.1in}
}



\end{frame}



\begin{frame}\frametitle{Standard Deviation}

\define{Variance}{ The variance of $X$, denoted by $\text{Var}(X)$ is deifined as 
$$\DBX{ \displaystyle \HLTW{\sigma_{_X}}=  \text{SD}(X):=\HLTW{\sqrt{\text{Var}(X)}}}$$\\
\vspace{-.1in}
}

$\text{Var}(X)$ is often denoted by $\sigma^2$.
\vspace{1.5in}
\end{frame}



\begin{frame}\frametitle{Properties of Expected Value and Variance}
 

Let $a, b\in \R$ are two constants where as $X$ is a random variable , then 
\begin{enumerate}
\item $E(a X+b)= a E(X)+ b$
\item $\text{Var}(a X+ b)= a^2 \text{Var}(X)$
\item $\text{SD}(a X+ b)= \abs{a} \text{SD}(X)$
\end{enumerate}
\vspace{2in}
\end{frame}

\section{ Moment Generating Function \HLTW{\text{(mgf)}}  of a Discrete Random Variable }
\TransitionFrame[antiquefuchsia]{\Large  Moment Generating Function \HLTW{\text{(mgf)}} of a Discrete Random Variable  }  


\begin{frame}\frametitle{Moment Generating Function (mgf)}
\vspace{-.05in}
\define{Moment Generating Function}{ The Moment Generating Function (mgf) of $X$, denoted by $\text{M}_{_X}(t)$ is deifined as 
$$\DBX{ \displaystyle \text{M}_{_X}(t):= E\left(\HLTW{e^{tX}} \right)},$$
whenever it exists. 
}
\begin{center}
\qBrd[4.2in]{antiquefuchsia!40}{ If the random variable $X$ has the probability mass funciton $\HLTY{\pmf_{_X}(x)}$ for all $ \HLTEQ[teal!40]{\HLTW{x}\in \support[X]}$, the support of X, then  assuming it exists\\
\begin{center}
\qBrd[3.4in]{applegreen!40}{$$M_{_X}(t):= E\left(\HLTW{e^{tX}} \right)= \sum_{\left\{\HLTEQ[teal!40]{\HLTW{x}\in \support[X]} \right\}} \HLTW{e^{tx}}  \; \HLTY{\pmf_{_X}(\HLTW{x})}. \vspace{-.2in}$$\\
\vspace{-.1in}
}
\end{center}
\vspace{.1in}
}
\end{center}

\end{frame}



\begin{frame}{Properties of a Moment Generating Function}
\qBrd[4.5in]{amethyst!40}{\sqBullet{amethyst}
If it exists, the moment generating function is unique for a random variable.  It means, no two random variable/distribution can have same moment geneting function.  Therefore, a distribution can be identified by the form of its moment generating function. 
}

\qBrd[4.5in]{teal!20}{\sqBullet{teal}\small
If it exists, the mgf  can be used to obtain the moments of a random variable in the following way: \\
\vspace{-.1in}
\begin{center}
\qBrd[4.2in]{applegreen!50}{
Assuming it exists
\qBrd[2in]{amber!20}{$\HLTW{ \frac{d}{dt}\left\{  M_X(t)\right\}}\Biggr|_{t=0}= \HLTW{E(X)}  $}
}\\
\vspace{.1in}
\qBrd[4.2in]{amber!50}{
Assuming it exists\qBrd[2.95in]{applegreen!20}{$ \HLTW{\frac{d^{k}}{dt^k}\left\{  M_X(t)\right\}}\Biggr|_{t=0}= \HLTW{E(X^k) } \text{ for }k=1 , 2, \ldots  $}
}
\end{center}
}
\end{frame}



\section{ A Few Examples}
\TransitionFrame[amber]{\Large  A Few Examples}  



\begin{frame}\frametitle{Example}
\vspace{-.1in}
\qbx[4.5in]{amethyst!50}{
The probability distribution of $X$, the number of daily network blackouts is given by
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline\hline
x    & 0    & 1    & 2    \\ \hline
$\pmf_{_X}(x)$ & 0.7 & 0.2 & 0.1 \\ \hline\hline
\end{tabular}
\end{center}
Find the Expected value and variance of the random variable {\bf X}.
}\\
\pause
\vspace{.1in}
\begin{minipage}{.48\textwidth} %
{\tiny 
{\bf Solution: } 
\qBrd[2.25in]{babyblue!40}{
\begin{eqnarray*}
\mu_{_X}= E(X)& =&  \sum_{x\in \{0,1,2\}} x \pmf_{_X}(x)\\
& = &  0 \times\pmf_{_X}(0)+ 1\times \pmf_{_X}(1)+  2\times \pmf_{_X}(2)\nonumber\\
& = & 0 \times 0.7 + 1\times 0.2+ 2 \times 0.1\nonumber\\
& = &  0.4  
 \end{eqnarray*}
  \vspace{-.1in}
                    }
}
\end{minipage} %
\begin{minipage}{.48\textwidth} %
\qBrd[2.22in]{babyblueeyes!50}{
\tiny
\begin{eqnarray*}
 E(X^2)& =&  \sum_{x\in \{0,1,2\}} x^2 \pmf_{_X}(x)\\
& = &  0^2 \times\pmf_{_X}(0)+ 1^2\times \pmf_{_X}(1)+  2^2\times \pmf_{_X}(2)\nonumber\\
& = & 0 \times 0.7 + 1\times 0.2+ 4 \times 0.1\nonumber\\
& = &  0.6 
 \end{eqnarray*}
 \vspace{-.2in}
                    }       
\end{minipage}
{\tiny Hence $\text{Var}(X):= E(X^2)- \left(E(X)\right)^2= 0.6-(0.4)^2= 0.6-0.16= 0.44$\\
The Standard Deviation $\sigma= \sqrt{\sigma^2}=\sqrt{\text{Var}(X)}=\sqrt{0.44}= 0.6633  $}


\vspace{.1in}
\end{frame}





\begin{frame}\frametitle{Example}
\vspace{-.1in}
\qbx[4.5in]{amethyst!50}{
The probability distribution of $X$, the number of daily network blackouts is given by
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline\hline
x    & 0    & 1    & 2    \\ \hline
$\pmf_{_X}(x)$ & 0.7 & 0.2 & 0.1 \\ \hline\hline
\end{tabular}
\end{center}
A small internet trading company estimates that each network
blackout results in a \$500 loss. Compute expectation and variance of this company's daily loss due to blackouts.
}\\
\pause
\vspace{.1in}
{\tiny The daily loss due to blackouts is given by $ h(X) = 500X$.  We need to find $E(h(X))$ and Varinace of $Var(h(X))$.}
\begin{minipage}{.48\textwidth} %
{\tiny 
{\bf Solution: } 
\qBrd[2.25in]{babyblue!40}{
\begin{eqnarray*}
\mu_{_X}= E(X)& =&  \sum_{x\in \{0,1,2\}} x \pmf_{_X}(x)\\
& = &  0 \times\pmf_{_X}(0)+ 1\times \pmf_{_X}(1)+  2\times \pmf_{_X}(2)\nonumber\\
& = & 0 \times 0.7 + 1\times 0.2+ 2 \times 0.1\nonumber\\
& = &  0.4  
 \end{eqnarray*}
  \vspace{-.2in}
                    }
}
\end{minipage} %
\begin{minipage}{.48\textwidth} %
\qBrd[2.22in]{babyblueeyes!50}{
\tiny
\begin{eqnarray*}
 E(X^2)& =&  \sum_{x\in \{0,1,2\}} x^2 \pmf_{_X}(x)\\
& = &  0^2 \times\pmf_{_X}(0)+ 1^2\times \pmf_{_X}(1)+  2^2\times \pmf_{_X}(2)\nonumber\\
& = & 0 \times 0.7 + 1\times 0.2+ 4 \times 0.1\nonumber\\
& = &  0.6 
 \end{eqnarray*}
  \vspace{-.2in}
                    }       
\end{minipage}
{\tiny Hence $\text{Var}(X):= E(X^2)- \left(E(X)\right)^2= 0.6-(0.4)^2= 0.6-0.16= 0.44$\\
The Standard Deviation $\sigma= \sqrt{\sigma^2}=\sqrt{\text{Var}(X)}=\sqrt{0.44}= 0.6633  $}


\vspace{.1in}
\end{frame}

\begin{frame}
\qbx[4.5in]{apricot!50}{
Let the pmf of a discrete is given as 
$$ \pmf_{_X}(x):= \frac{1}{2^x} \text{ for } x= 1, 2, 3, \ldots  $$
}
\vspace{2in}
\end{frame}




\begin{frame}\frametitle{Exercises on Computing $E(X)$,  $Var(X)$, $MGF$}
\qBrd[4.6in]{airforceblue!40}{
Find $E(X)$ and $Var(X)$, where $X$ is the outcome when we roll a fair die.
}
\vspace{3in}

\end{frame}


\begin{frame}\frametitle{Exercises on Computing $E(X)$,  $Var(X)$, $MGF$}
\qBrd[4.6in]{olive!30}{
 We say that $\Indicator{A}{x}$ is an indicator function for the event $A$ if 
$$  \Indicator{A}{x}:=
 \begin{cases}
 1 & \text{if } x\in A\\
  0 & \text{if } x\notin A.
\end{cases}  $$
Obtain $E(  \Indicator{A}{X})$ and $ \text{Var}\left( \Indicator{A}{x} \right).$
}
\vspace{1.5in}


\end{frame}



%
%
%\section{Binomial Distribution  }
%\TransitionFrame[bittersweet]{\Large Binomial Distribution  }
%
%
%\begin{frame}{Binomial Distribution}
%\begin{itemize}
%\qBrd[4.2in]{olive!30}{
%\item A {\bf Bernoulli experiment} is a random experiment, the outcome of which can be classified in one of two mutually exclusive and
%exhaustive ways, say,  ``1=success" or ``0=failure." Let $ Y$ be the
%number of success on a Bernoulli trial, then $Y$ is called the Bernoulli
%random variable. 
%}
%\vspace{.1in}\\
%\qBrd[4.2in]{teal!30}{
%\item If a sequence of $n$ independent Bernoulli trials is performed under the same condition, we call it a set of n Bernoulli trials a Binomial experiment.
%}
%\end{itemize}
%\end{frame}
%
%
%
%
%\begin{frame}{Binomial Distribution}
%\define{Binomial Experiment}{
%An experiment is called a Binomial experiment if it satisfies the following 4 conditions:
%\begin{itemize}
%\item The experiment consists of $n$ Bernoulli trials.
%\item Each trial results in a success (S) or a failure (F).
%\item The trials are independent.
%\item The probability of a success, $p$,  is fixed throughout $n$ trials.
%\end{itemize}
%}
%\end{frame}
%
%
%\begin{frame}{Binomial Distribution $\text{Binomial}(n,p)$}
%\begin{enumerate}
%\item Given a Binomial experiment consisting of n Bernoulli trials with
%success probability p, the Binomial random variable X associated
%with this experiment is defined as the number of successes among the
%n trials.
%\item The random variable X has the Binomial Distribution with
%parameters n and p; denoted by$ X\sim Binomial(n, p).$
%
%\item The behavior of Binomial Distribution with different $n$ and $p$.
%
%\end{enumerate}
%
%\end{frame}
%
%
%
%
%\begin{frame}{Binomial Distribution $\text{Binomial}(n,p)$}
%\define{Binomial Distribution}{
%Let $p\in (0,1)$, then the probability mass function of $\text{Binomial}(n, p)$ is given by
%$$\HLTEQ[amethyst!50]{ \HLTW{\displaystyle \HLTY{\pmf(x)}:= {n \choose x} p^x (1-p)^{n-x}}}, \text{ for } x \in \support[X],  \text{where } \HLTEQ[teal!30]{ \support[X]=\{ 0, 1, \ldots,  n\}}$$\\
%}
%\vspace{.1in}
%
%\qbx[4.6in]{babyblue!40}{
%$\Row{\qBrd[2in]{amethyst!50}{\text{Mean}\\
%\HLTW{E(X)= n p} },  \qBrd[2in]{amethyst!50}{\text{Variance}\\\HLTW{\text{VAR}(X)= n p (1-p)}} } $
%}
%
%\end{frame}
%
%
%
%\begin{frame}{Expected Value of Binomial Distribution}
%\begin{eqnarray}
%E(X)& := &  \sum_{y\in \support[X]} y\; \pmf_{_X}(y) \nonumber\\
%& = &  \sum_{y=0}^{n} y {n\choose y}p^y(1-p)^{n-y}  \nonumber\\
%& =& (1-p)^{n}  \sum_{y=0}^{n}  y {n\choose y}\left( \frac{p}{1-p}\right)^y   \nonumber\\
%& =& (1-p)^{n}  \frac{np}{(1-p)^n} \nonumber\\
%& =& np
%\end{eqnarray}
%
%\end{frame}
%
%
%\begin{frame}{Expected Value of Binomial Distribution}
%\begin{eqnarray}
%E(X^2)& := &  \sum_{y\in \support[X]} y^2\; \pmf_{_X}(y) \nonumber\\
%& = &  \sum_{y=0}^{n} y^2 {n\choose y}p^y(1-p)^{n-y}  \nonumber\\
%& =& (1-p)^{n}  \sum_{y=0}^{n}  y^2 {n\choose y}\left( \frac{p}{1-p}\right)^y   \nonumber\\
%& =& (1-p)^{n}  \frac{mp+ n(n-1)p^2}{(1-p)^n} \nonumber\\
%& =& np+ n(n-1)p^2
%\end{eqnarray}
%
%$Var(X)= E(x^2)- (E(X))^2= np+n(n-1)p^2- n^2p^2= np-np^2= np(1-p). $
%
%\end{frame}
%
%
%
%\begin{frame}{Expected Value of Binomial Distribution}
%\begin{eqnarray}
%M_X(t)& := &  \sum_{y\in \support[X]} e^{ty}\; \pmf_{_X}(y) \nonumber\\
%%& = &  \sum_{y=0}^{n} e^{ty} {n\choose y}p^y(1-p)^{n-y}  \nonumber\\
%& =& (1-p)^{n}  \sum_{y=0}^{n}  e^{ty} {n\choose y}\left( \frac{p}{1-p}\right)^y   \nonumber\\
%& =& (1-p)^{n}  \sum_{y=0}^{n}  {n\choose y}\left( \frac{pe^{t}}{1-p}\right)^y   \nonumber\\
%& =& (1-p)^{n}  \sum_{y=0}^{n}  e^{ty} {n\choose y}\left( \frac{p}{1-p}\right)^y   \nonumber\\
%& =& (1-p)^{n} \left( 1+  \frac{pe^t}{1-p} \right)^n = \left( 1-p+pe^t\right)^n  \nonumber\\
%\end{eqnarray}
%
%
%
%\end{frame}
%
%
%
%
%\begin{frame}\frametitle{Example}
%\vspace{-.1in}
%\qbx[4.5in]{amber!50}{
%\Exmpl{amber}{} Five fair coins are  flipped.  If the outcomes are assumed independent.
%\begin{enumerate}
%\item Find the probability mass function of the number of heads obtained.
%\item Find the probability that at least 3 heads are obtained.
%\item Find the probability that at most 2 heads are obtained.
%\end{enumerate}
%}\\
%\pause
%\vspace{.1in}
%{\tiny 
%{\bf Solution: }
%Let $X$ = The number of heads in 5 tossed coins. $X\sim Binomial(n=5, p=0.5)$.
%\begin{enumerate}
%\item $P(X = 0) =0.5^5 = 0.0313$
%\item $P(X = 1) ={5 \choose 1}0.5^5 =0.1563$
%\item $P(X = 2) ={5 \choose 2}0.5^5 =0.3125$
%\item $P(X = 3) ={5 \choose 3}0.5^5 =0.3125$
%\item $P(X = 4) ={5 \choose 4}0.5^5 =0.1563$
%\item $P(X = 0) ={5 \choose 5}0.5^5 = 0.0313$
%\end{enumerate}
%}
%\end{frame}
%
%
%\begin{frame}\frametitle{Example}
%\vspace{-.1in}
%\qbx[4.5in]{amethyst!50}{
%\Exmpl{amethyst}{} It is known that screws produced by a certain company will be defective with probability .01, independently of each other. The company sells the screws in packages of 10 and offers a money-back guarantee that at most 1 of the 10 screws is defective. What proportion of packages sold must the company replace? Use the
%Binomial Calculator or Statistical Tables.
%}\\
%\vspace{1.5in}
%
%\end{frame}
%
%
%\begin{frame}\frametitle{Example}
%\vspace{-.1in}
%\qbx[4.5in]{olive!50}{
%\Exmpl{olive}{} The following gambling game, known as the wheel of fortune (or chuck-a-luck), is quite popular at many carnivals and gambling casinos: A player bets on one of the numbers 1 through 6. Three dice are then rolled, and if the number bet by the player appears i times, i = 1; 2; 3, then the player wins i units; if the number bet by the
%player does not appear on any of the dice, then the player loses 1 unit. Is this game fair to the player?
%}\\
%\vspace{1.5in}
%\end{frame}
%
%
%
%\section{Poisson Distribution  }
%\TransitionFrame[bittersweet]{\Large Poisson Distribution  }
%\begin{frame}\frametitle{Poisson Distribution}
%The Poisson distribution models the number of occurrences of an
%event when there is a known average rate per unit time or space $\lambda$.
%
%\define{Poisson Distribution}{
%The requirements for a Poisson distribution are that:
%\begin{enumerate}
%\item no two events can occur simultaneously,
%\item events occur independently in different intervals, and
%\item the expected number of events in each time interval remain constant.
%\end{enumerate}
%}
%\end{frame}
%
%
%\begin{frame}\frametitle{Poisson Distribution: pmf, Expected Value }
%The Poisson distribution models the number of occurrences of an
%event when there is a known average rate per unit time or space $\lambda$.
%
%\define{Poisson Distribution: pmf, Expected Value}{
%The requirements for a Poisson distribution are that:
%\begin{enumerate}
%\item The probability mass function of $\text{Poisson}(\lambda)$ is given by $$ p(x) = \frac{e^{-\lambda}\lambda^x}{x!} \text{ for } x = 0, 1, 2, 3, \ldots $$
%\item If $X\sim Poisson(\lambda)$, then  $E(X)= \lambda,$ and $\text{Var}(X)= \lambda$.
%\end{enumerate}
%}
%\end{frame}
%
%
%
%
%
%\begin{frame}{Expected Value of Binomial Distribution}
%\begin{eqnarray}
%M_X(t)& := &  \sum_{y\in \support[X]} e^{ty}\; \pmf_{_X}(y) \nonumber\\
%& = &  \sum_{y=0}^{\infty} e^{ty} \frac{e^{-\lambda} \lambda^y}{y!} \nonumber\\
%& = &  e^{-\lambda}\sum_{y=0}^{\infty}  \frac{ \left( \lambda e^t\right)^y}{y!} \nonumber\\
%& = &  e^{\lambda e^t-\lambda}
%\end{eqnarray}
%
%
%
%\end{frame}
%
%
%\begin{frame}\frametitle{A few Examples of Poisson Distribution }
%\vspace{-.1in}
%\qbx[4.5in]{olive!50}{
%\Exmpl{olive}{} The number of customers arriving at a service counter within one-hour period.
%}\\
%\vspace{.1in}
%\qbx[4.5in]{teal!50}{
%\Exmpl{olive}{} The number of typographical errors in a book counted per page.
%}\\
%\vspace{.1in}
%\qbx[4.5in]{amethyst!50}{
%\Exmpl{amethyst}{} The number of email messages received at the technical support center daily.
%}\\
%\vspace{.1in}
%\qbx[4.5in]{babyblue!50}{
%\Exmpl{babyblue}{} The number of traffic accidents that occur on a specific road  during a month.
%}\\
%\vspace{.2in}
%\end{frame}
%
%
%
%
%
%\begin{frame}\frametitle{A Few Examples of Poisson Distribution }
%\vspace{-.1in}
%\qbx[4.5in]{olive!50}{
%\Exmpl{olive}{} Messages arrive at an electronic message center at random times, with an average of 9 messages per hour. 
%\begin{enumerate}
%\item What is the probability of receiving exactly five messages during the next hour?
%\item What is the probability that more than 10 messages will be received within the next two hours?
%\end{enumerate}
%}\\
%
%
%\vspace{.5in}
%{\tiny 
%\begin{enumerate}
%\item The number of messages received in an hour, $X$ is modeled by
%Poisson distribution with $\lambda =  9$, i.e.  $X\sim \text{Poisson}(9)$.
%$P(X=5)= \frac{9^5 \exp(-9)}{5!}$
%\item The number of messages received within a 2-hour period, $Y$ is
%another Poisson distribution with  $Y= (2)(9) =18$, i.e. $Y\sim  \text{Poisson}(18)$.
%$P(Y > 10) =1- P(Y\leq 10 )= ...= 0.9696$
%\end{enumerate}
%
%}
%
%\end{frame}
%
%
%
%
%\begin{frame}\frametitle{Group Work }
%\begin{enumerate}
%\item Develop a real life example in which you can easily apply:
%\begin{enumerate}
%\item  Group 1: Poisson distribution.
%\item  Group 2: Binomial distribution.
%\item Group 3: Poisson distribution
%\end{enumerate}
%\item  In each case, propose two problems which can be solved using the Statistical Calculator.
%\item Can you propose an idea in which you can mix both distributions?
%(extra)
%\end{enumerate}
%
%\end{frame}
%
%
%
%
%
%\section{Geometric Distribution   }
%\TransitionFrame[bittersweet]{\Large Geometric Distribution}
%
%
%
%\begin{frame}\frametitle{Geometric Distribution}
%\vspace{-.1in}
%\begin{enumerate}
%\item Suppose that independent trials, each having a probability $p$,
%$0 < p <1$, of being a success, are performed until a success occurs.
%\item Example: The first head in tossing coin several times.
%\item Then, Geometric distribution models the number of trials performed until a success occurs.
%\end{enumerate}
%
%\define{Geometric Distribution}{
%The probability mass function of $Geometric(p)$ is given by
%$$p(x) = (1-p)^{x-1}p \text{ for } x = 1, 2, 3, \ldots, $$
%}
%
%\qBrd[4.7in]{olive!40}{
%\sqBullet{olive} If $X\sim \text{Geometric}(p)$ then $E(X)= \frac{1}{p}$, and $\text{Var}(X)= \frac{1-p}{p^2}$
%}
%
%
%\end{frame}
%
%
%
%
%
%
%
%
%
%\begin{frame}{Expected Value of Binomial Distribution}
%\begin{eqnarray}
%M_X(t)& := &  \sum_{y\in \support[X]} e^{ty}\; \pmf_{_X}(y) \nonumber\\
%& = &  \sum_{y=1}^{\infty } e^{ty}  (1-p)^{y-1}p \nonumber\\
%& = & p   \sum_{z=0}^{\infty } e^{tz+t}  (1-p)^{z} \nonumber\\
%& = & pe^t   \sum_{z=0}^{\infty } \left((1-p)e^{t}\right)^z\nonumber\\
%& = & \frac{pe^t}{1-(1-p)e^{t}} 
%\end{eqnarray}
%
%
%
%\end{frame}
%
%
%
%\begin{frame}\frametitle{Geometric Distribution: Example}
%\vspace{-.1in}
%\qbx[4.5in]{olive!50}{
%\Exmpl{olive}{}  Suppose that the probability of engine malfunction during any one-hour period is p = 0.02. Find the probability that a given engine will survive two hours.
%}\\
%\pause
%
%\vspace{.1in}
%{\tiny Solution: \\
%Letting $Y$ denote the number of one-hour intervals until the first
%malfunction, we have 
%\begin{eqnarray}
%& & P(\text{Survival for Next Two Hours})\nonumber\\
%& =&  P(Y\geq 3)\nonumber\\
%& =& 1- P(Y\leq 2)\nonumber\\
%& = &1-   \sum_{y=1}^{2}p(y)\nonumber\\
%& = &1-   \left\{ p(1)+p(2)\right\}\nonumber\\
%& =& 1-0.02-0.98\times 0.02\nonumber\\
%& =& 0.9604\nonumber
%\end{eqnarray}
%}
%\qBrd[4.5in]{babyblue!70}{
%\HLTW{\text{Exercise}}
% Find the mean and standard deviation of Y.
%}
%
%\end{frame}
%
%
%
%
%\section{Negative Binomial Distribution }
%\TransitionFrame[bittersweet]{\Large Negative Binomial Distribution  }
%
%
%\begin{frame}
%\begin{enumerate}
%\item Suppose that independent trials, each having probability  $p$,
%$0 < p < 1$, of being a success are performed until a total of r
%successes is accumulated.
%\item  Example: The third head in tossing coin several times.
%
%\item  Then, Negative Binomial distribution models the number of trials
%performed until a the rth success occurs.
%\end{enumerate}
%
%\define{Negative Binomial Distribution}{
%The probability mass function of Negative Binomial RV, denoted by $\text{Negative-Binomial}(r, p)$ is given by
%$$p(x) ={{x-1}\choose{r-1}}p^{r-1} (1-p)^{x-r} \text{ for } x = r+1, r+2,r+ 3, \ldots, $$
%}
%
%\qBrd[4.7in]{olive!40}{
%\sqBullet{olive} If $X\sim \text{Negative-Binomial}(r, p)$ then $E(X)= \frac{r}{p}$, and $\text{Var}(X)= \frac{r(1-p)}{p^2}$
%}
%
%
%\end{frame}
%
%
%\begin{frame}\frametitle{Geometric Distribution: Example}
%\vspace{-.1in}
%\qbx[4.5in]{amethyst!50}{
%\Exmpl{amethyst}{}{
%A machine produces 1\% defective parts. Using the statistical calculator, calculate the probability that
%\begin{enumerate}
%\item 10 parts have to be selected until to get 2 defective parts.
%\item Between 20 to 25 parts have to be selected to get 2 defective parts.
%\end{enumerate}
%}  
%}\\
%\pause
%
%\vspace{.1in}
%{\tiny Solution: \\
%Letting $Y$ denote the number of 
%\begin{eqnarray}
%AA
%\end{eqnarray}
%}
%\qBrd[4.5in]{babyblue!70}{
%\HLTW{\text{Exercise}}
% Find the mean and standard deviation of Y.
%}
%
%\end{frame}
%
%



\TransitionFrame[antiquefuchsia]{\Large Questions?  }
 
 
\end{document}
