\documentclass[compress]{beamer}
\mode<presentation>
\setbeamercovered{transparent}
\usetheme{Warsaw}
%\useoutertheme{smoothtree}
\usepackage{multirow}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{xmpmulti}
\usepackage{multicol}
\usepackage{colortbl}
\usepackage{diagbox}
\usepackage{makecell}

%\setbeamersize{text margin left=.25 in,text margin right=.25 in}
\setbeamersize{text margin left=.15 in,text margin right=.15 in}
\usepackage[authoryear]{natbib}


\usepackage{epstopdf}
\usepackage{xcolor}
\usepackage{latexcolors}
%\usepackage[dvipsnames]{xcolor}
\definecolor{antiquebrass}{rgb}{0.8, 0.58, 0.46}
\definecolor{babyblueeyes}{rgb}{0.63, 0.79, 0.95}
\definecolor{babyblue}{rgb}{0.54, 0.81, 0.94}
\definecolor{bistre}{rgb}{0.24, 0.17, 0.12}
\definecolor{brightlavender}{rgb}{0.75, 0.58, 0.89}
\definecolor{bulgarianrose}{rgb}{0.28, 0.02, 0.03}
%\definecolor{slateblue}{rgb}{0.56, 0.74, 0.56}
\definecolor{cordovan}{rgb}{0.54, 0.25, 0.27}
\definecolor{darkbyzantium}{rgb}{0.36, 0.22, 0.33}

%\setbeamercolor{structure}{fg=byzantium!70, bg= black!60}
\setbeamercolor{structure}{fg=slateblue!70, bg= black!60}






\usepackage{tikz}
\usetikzlibrary{shadows,calc}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes.symbols}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
%%%%%%%%% shaddow image %%%%%
% some parameters for customization
\def\shadowshift{3pt,-3pt}
\def\shadowradius{6pt}
\colorlet{innercolor}{black!60}
\colorlet{outercolor}{gray!05}
% this draws a shadow under a rectangle node
\newcommand\drawshadow[1]{
\begin{pgfonlayer}{shadow}
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.south west)+(\shadowshift)+(\shadowradius/2,\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) circle (\shadowradius);
    \shade[outercolor,inner color=innercolor,outer color=outercolor] ($(#1.north east)+(\shadowshift)+(-\shadowradius/2,-\shadowradius/2)$) circle (\shadowradius);
    \shade[top color=innercolor,bottom color=outercolor] ($(#1.south west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) rectangle ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$);
    \shade[left color=innercolor,right color=outercolor] ($(#1.south east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$);
    \shade[bottom color=innercolor,top color=outercolor] ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$);
    \shade[outercolor,right color=innercolor,left color=outercolor] ($(#1.south west)+(\shadowshift)+(-\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north west)+(\shadowshift)+(\shadowradius/2,-\shadowradius/2)$);
    \shade[outercolor,right color=innercolor,left color=innercolor] ($(#1.north west)+(-\shadowradius/12,\shadowradius/12)$) rectangle ($(#1.south east)+(\shadowradius/12,-\shadowradius/12)$);%Frame
    \filldraw ($(#1.south west)+(\shadowshift)+(\shadowradius/2,\shadowradius/2)$) rectangle ($(#1.north east)+(\shadowshift)-(\shadowradius/2,\shadowradius/2)$);
\end{pgfonlayer}
}
% create a shadow layer, so that we don't need to worry about overdrawing other things
\pgfdeclarelayer{shadow} 
\pgfsetlayers{shadow,main}
% Define image shadow command
\newcommand\shadowimage[2][]{%
\begin{tikzpicture}
\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[#1]{#2}};
\drawshadow{image}
\end{tikzpicture}}
\usepackage{calligra}

\DeclareMathOperator*{\argmax}{Arg\,max}
\DeclareMathOperator*{\argmin}{Arg\,min}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert }
\newcommand{\bbetaHat}{ \widehat{\bbeta}}
\newcommand{\bbetaLSE}{ \widehat{\bbeta}_{_{\text{LSE}}}}
\newcommand{\bbetaMLE}{ \widehat{\bbeta}_{_{\text{MLE}}}}
\newcommand{\sqBullet}[1]{  {\tiny \tiny \tiny \qBoxCol{#1!60}{ }} }
%***************
%\newtheorem{thm}{Theorem}
\input{../LatexSupportFiles/definition_include}
\input{../LatexSupportFiles/BoxDef}
\input{../LatexSupportFiles/MatrixDef}











\title{  STAT 320: Principles of Probability\\ {\color{black}  Unit 7: Bivariate \& Multivariate Random Vectors}}

\author[UAEU]
{United Arab Emirates University}
\institute[UAEU] % (optional, but mostly needed)
{
  \inst{Department of Statistics}%
  %Indian Institute of Management,  Udaipur\\
  \vspace{0.1in}

  
}

\date{}


\newcommand{\Xnew}{ \HLTEQ[orange]{X_{_{\text{i}}}} }
\newcommand{\Ynew}{ \HLTEQ[orange]{Y_{_{\text{i}}}} }

%\date{\today}

\AtBeginSection[]
{
  \begin{frame}{Inhalt}
 % \begin{multicols}{1}
	\frametitle{Outline}
    \tableofcontents[currentsection]
  %  \end{multicols}
  \end{frame}
}

\begin{document}
\maketitle

%\begin{frame}{Outline}
%%\begin{multicols}{}
%  \tableofcontents
%%\end{multicols}
%\end{frame}

%\section{Introduction to DSBA 2023}
%
%
%\begin{frame}
%\qBoxCol{blue!30}{
%\begin{center} Course  Website \end{center}
%\qbx[4.2in]{teal!40}{\sqBullet{teal} \color{blue} $ \href{https://sites.google.com/iimu.ac.in/dsba2023e/home}{https://sites.google.com/iimu.ac.in/dsba2023e/home}$
%}\\
%\qbx[3.0in]{green!40}{ \sqBullet{green} Regular Announcements.
%}\\
%\qbx[3.0in]{olive!40}{\sqBullet{olive}  Slides and other materials.
%}
%}
%
%\pause
%\qBoxCol{blue!30}{
%\sqBullet{blue}
%You can contact the instructor at {\it subhadip.pal@iimu.ac.in} and schedule for office hours.  
%}
%\pause
%\qBoxCol{olive!30}{
%\sqBullet{olive}
%Mr. Praveen Kumar has been assigned as Teaching Assistant (TA) for this course.  His email I'd is:  {\it praveen.kumar@iimu.ac. }
%}
%
%
%\end{frame}
%


%
%\begin{frame}{Course Outline}
%\hspace{-.1in}\qBoxCol{blue!35}{
%% Please add the following required packages to your document preamble:
%% \usepackage{booktabs}
%\begin{table}[]
%\begin{tabular}{@{}lll@{}}
%\toprule
%         & Topics                                                & Dataset or Case                                    \\ \midrule \midrule
%\rowcolor{blue!20}     \multicolumn{1}{|l|}{1-2}   & \multicolumn{1}{l|}{Overview of Data Science}        & \multicolumn{1}{l|}{Household Data}                \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{3-5}   & \multicolumn{1}{l|}{Data Visualization}              & \multicolumn{1}{l|}{Global Super Store }       \\ \midrule
%\rowcolor{blue!20} 
%\multicolumn{1}{|l|}{6}     & \multicolumn{1}{l|}{Introduction to R/ JMP}          & \multicolumn{1}{l|}{}                              \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{7}     & \multicolumn{1}{l|}{Regression Analysis}             & \multicolumn{1}{l|}{Display \& Liquor Sales} \\ \midrule
%\rowcolor{blue!20} 
%\multicolumn{1}{|l|}{8}     & \multicolumn{1}{l|}{Multiple Regression}             & \multicolumn{1}{l|}{}                              \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{9}     & \multicolumn{1}{l|}{Dealing with Nominal Covariates} & \multicolumn{1}{l|}{Gender Divide}                 \\ \midrule
%\rowcolor{blue!20} 
%\multicolumn{1}{|l|}{10}    & \multicolumn{1}{l|}{Regression Diagonistics}         & \multicolumn{1}{l|}{}                              \\ \midrule
%\rowcolor{purple!20} 
%\multicolumn{1}{|l|}{11-12} & \multicolumn{1}{l|}{Project Presentations}            &\multicolumn{1}{l|}{}          \\\midrule \bottomrule
%\end{tabular}
%\end{table}
%}
%\end{frame}


%\begin{frame}{Case Study }
%\qBoxCol{teal!40}{\vspace{1in}\begin{center}\sqBullet{teal} \Large Case: Liquor sales and display space \end{center}
%\vspace{1in}
%}\\
%\end{frame}





\begin{frame}
\begin{enumerate}
 \qBrd[4.1in]{babyblue!70}{\item[\sqBullet{babyblue}]
 In this part of the course we will focus the procedures to study multiple random variables together. 
 }\\
 \vspace{.1in}
\qBrd[4.1in]{amethyst!40}{ \item[\sqBullet{amethyst}]
  Let $X_1, X_2, \ldots , X_n $ be random variables,  to study their probabilistic propertis jointly we construct the Myltivariate Random Vector 
  $$  \HLTW{\utilde{\bX} := \left(  X_1, X_2, \ldots , X_n \right)^T  }$$ 
 }
 \\
 \vspace{.1in}
\qBrd[4.1in]{babyblue!70}{ \item[\sqBullet{babyblue}]
  However,  we will focus only the Bi-Variate case with two random variables  that we will mostly denote by $\HLTW{(X,Y)}$
 }
 
 
\end{enumerate}

 
 
\end{frame}




\section{Discrete Multivariate Random Variables }
\TransitionFrame[bittersweet]{\Large Discrete Multivariate Random Variables  }
\begin{frame}\frametitle{Bivariate Discrete Randmo Variable}
\begin{enumerate}
\qBrd[4.1in]{babyblue!70}{ \item[\sqBullet{babyblue}]
We call $(X,Y)$ to be discrete bivariate random vector is both the random variables $X$ and $Y$ are discrete in nature.  Also the corresponding support 
$$\HLTW{\displaystyle \support[X,Y]:=\left\{ (x,y)\in \R^2 :   (x,y) \text{ is a possible value of} (X,Y) \right\}}$$
 }
 \\
 \vspace{.1in}
 \qBrd[4.1in]{applegreen!50}{ \item[\sqBullet{applegreen}]
 $(X,Y)$ is a bovariate discrete random variable if the corresponding support $\HLTW{\support[X,Y]}$ is a discrete set. 
 }
 \end{enumerate}

\end{frame}


%
%
%\begin{frame}\frametitle{Joint Probability Mass Function}
%A
%\end{frame}


\begin{frame}{Joint p.m.f/ Support/ Diagram of support }

\qBrd[4.6in]{babyblue!70}{
\qBrd[2.35in]{babyblue!40}{Probability Mass Function (pmf)}
For a discrete random vector  $(X,Y)$, we define the probability mass function (pmf) $\pmf_{{_X},{_Y}}(x,y)$ of $X$ by 
$$\HLTW{\HLTY{\pmf_{{_X},{_Y}}(x,y)}= P(X=x, Y=y ) }\text{ for all } \HLTW{ (x,y)\in \support[X,Y]}$$
}

\vspace{1.5in}



\end{frame}









\begin{frame}\frametitle{Joint Cumulative Distribution Function}

\define{Bivariate CDF}{
Let X, Y be two discrete random variables. The joint cumulative distribution function is given by\\
\vspace{-.1in}
\begin{center}
\qBrd[3in]{antiquefuchsia!40}{
$\displaystyle \HLTW{ F_{_{X,Y}}(\HLTY{x},\HLTY{y})}:= P\left(X\leq \HLTY{x}, Y\leq \HLTY{y} \right) .$}
\vspace{.1in}
\end{center}
}
\vspace{2in}
\end{frame}


\begin{frame}\frametitle{Joint CDF (Discrete Random Variable)}

$$\qBrd[3.0in]{teal!40}{ \Large  Joint CDF from Joint p.m.f.   }$$
\vspace{- .2in}
\qbx[4.65in]{olive!30}{
If  the joint probability mass function of two random variables (X,Y) on the support $\HLTW{\support[X,Y]}$  is  \qBrd[2in]{babyblue!40}{$\pmf_{_{X,Y}}(x, y) = P(X = x, Y = y).$} then \\
\qBrd[4.45in]{antiquefuchsia!60}{
$$\vspace{-.05in}    \HLTW{ F_{_{X,Y}}(\HLTY{x},\HLTY{y})} = \mathop{\sum\sum}_{\left\{  \Col{{ \HLTW{s}\leq \HLTY{x}, \HLTW{t} \leq \HLTY{y} }, {\text{ where } (s,t)\in \HLTW{\support[ X,Y]}}}  \right\}} \pmf_{_{X,Y}}\left(\HLTW{s}, \HLTW{t}\right)$$
\vspace{-.1in}
}
\vspace{.05in}
}
\end{frame}




\begin{frame}\frametitle{Marginal  Distributions for Discrete Random Vector}
\qbx[4.2in]{babyblue!40}{
The marginal probability mass function of $X$ is given by
$$\pmf_{_X}(\HLTY{x})=  \sum_{\left\{\HLTW{t}: (\HLTY{x}, \HLTW{t})\in \support[XY]\right\}}  \pmf_{_{X,Y}}(\HLTY{x}, \HLTW{t})$$
}
\vspace{.1in}

\qbx[4.2in]{babyblueeyes!40}{
The marginal probability mass function of $X$ is given by
$$\pmf_{_Y}(\HLTY{y})=  \sum_{\left\{\HLTW{s}: (\HLTW{s},\HLTY{y})\in \support[XY]\right\}}  \pmf_{_{X,Y}}(\HLTW{s}, \HLTY{y})$$
}

\end{frame}


\begin{frame}

\end{frame}



\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
\Exmpl{amethyst}{}  Suppose two cards are drawn at random without replacement from a deck of 4 cards numbered 1, 2, 3, 4. Let X be the number on the first card and Y be the number of the second card.
\begin{enumerate}[a).]
\item Find the joint probability function of X and Y .
\item  Find the marginal probability function of X.
\item  Find the marginal probability function of Y .
\end{enumerate}
}\\
%\pause
%\vspace{1.7in}
{\tiny {:}  
\begin{table}[!ht]
\centering
\begin{tabular}{ |c|| c| c| c| c|| c| } 
\toprule
\diagbox{Y}{X} & \makecell{ 1}& \makecell{2} & \makecell{3} & \makecell{4} 
& \makecell{Marginal of Y}  \\ 
\midrule
\midrule
1 &  0 & $\frac{1}{12}$ &  $\frac{1}{12}$ &  $\frac{1}{12}$& $\frac{1}{4}$ \\\hline
2 &   $\frac{1}{12}$& 0 &  $\frac{1}{12}$ &  $\frac{1}{12}$&  $\frac{1}{4}$ \\\hline
3&   $\frac{1}{12}$ &  $\frac{1}{12}$ & 0 &  $\frac{1}{12}$ &  $\frac{1}{4}$ \\\hline
4 &   $\frac{1}{12}$ &  $\frac{1}{12}$ &  $\frac{1}{12}$ & 0 &  $\frac{1}{4}$ \\\hline\hline
Marginal of X &   $\frac{1}{4}$ &  $\frac{1}{4}$ &  $\frac{1}{4}$ &  $\frac{1}{4}$ &  \\
\bottomrule
\end{tabular}
\end{table}
}
\end{frame}





\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{applegreen!40}{
\Exmpl{applegreen}{}  A fair coin is 
flipped three times. Let X denotes the number of heads to occur in the first two flips, a and let Y denotes the number of heads to
occur in the last two flips. 
\begin{enumerate}[a).]
\item Find the joint probability function of $(X,Y)$
\item  and the marginal probability functions of $X$, and $Y$.
\item  Calculate  $P(X = Y)$.
\end{enumerate}
}\\
%\pause
%\vspace{1.7in}
{\tiny {:}  
\begin{table}[!ht]
\centering
\begin{tabular}{ |c|| c| c| c| c||  } 
\toprule
\diagbox{Y}{X} & \makecell{ 0}& \makecell{1} & \makecell{2} 
& \makecell{Marginal of Y}  \\ 
\midrule
\midrule
0&   $\frac{1}{8}$ &  $\frac{1}{8}$ &  0 & $\frac{1}{4}$\\\hline
1&   $\frac{1}{8}$&   $\frac{2}{8}$ &  $\frac{1}{8}$ &  $\frac{2}{4}$\\\hline
2&   0 &  $\frac{1}{8}$ &   $\frac{1}{8}$ & $\frac{1}{4}$  \\\hline\hline
Marginal of X &   $\frac{1}{4}$ &  $\frac{2}{4}$ &  $\frac{1}{4}$ & \\
\bottomrule
\end{tabular}
\end{table}
\qBrd{applegreen!30}{
$$ P(X=Y)=   \sum_{\{(\HLTY{x},\HLTW{y})\in \support[XY]: x=y\}}  \pmf_{_{X,Y}}(\HLTY{x}, \HLTW{y})= \pmf_{_{X,Y}}(0,0)+ \pmf_{_{X,Y}}(1,1)+\pmf_{_{X,Y}}(2,2)= \frac{1}{2}. $$
}


}
\end{frame}






\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{teal!40}{
\Exmpl{teal}{} Suppose that 3 balls are randomly selected from an urn containing 3 red, 4 white, and 5 blue balls. If we let X and Y denote respectively, the
number of red and white balls chosen. Find the joint probability mass
function of X and Y.
}\\
%\pause
%\vspace{1.7in}
{\tiny {:}  
{\tiny {:}  
\begin{table}[!ht]
\centering
\begin{tabular}{ |c|| c| c| c| c|| c| } 
\toprule
\diagbox{Y}{X} & \makecell{ 0}& \makecell{1} & \makecell{2} & \makecell{3} 
& \makecell{Marginal of Y}  \\ 
\midrule
\midrule
0 &   $\frac{10}{220}$ &  $\frac{30}{220}$ &  $\frac{15}{220}$&$\frac{1}{220}$& $\frac{56}{220}$ \\\hline
1&   $\frac{40}{220}$& $\frac{60}{220}$&  $\frac{12}{220}$ & 0&  $\frac{112}{220}$ \\\hline
2 &   $\frac{30}{220}$ &  $\frac{18}{220}$ & 0 &  0 &  $\frac{48}{220}$ \\\hline
3 &   $\frac{4}{220}$ &  0 & 0 & 0 &  $\frac{4}{220}$ \\\hline\hline
Marginal of X &   $\frac{84}{220}$ &  $\frac{108}{220}$ &  $\frac{27}{220}$ &  $\frac{1}{220}$ &  \\
\bottomrule
\end{tabular}
\end{table}
}
}
\end{frame}




\section{Continuous Multivariate Random Variables	 }
\TransitionFrame[bittersweet]{\Large Continuous Multivariate Random Variables  }


\begin{frame}
\qbx[4.5in]{amber!40}{
We say that X and Y are jointly continuous if there exists a function \HLTW{f_{_{X,Y}}(x, y)}, defined
for all real x and y, having the property that, for every set $C$  of pairs of real numbers (that is, C is a set in the two-dimensional plane), 
 $$P\left(\HLTW{(X,Y) \in C}\right)=  \iint\limits_{\{(x,y)\in C\}}  f_{_{X,Y}}(\HLTY{s}, \HLTW{t})ds \;dt$$
}\\
\vspace{.1in}
\qbx[4.5in]{applegreen!40}{
\HLTW{f_{_{X,Y}}(x, y)} is called the joint probability density function of the random vector $(X,Y)$.
}


\end{frame}


\begin{frame}\frametitle{Joint CDF for Continuous Random Vector}

\define{Bivariate CDF}{\tiny 
Let X, Y be two discrete random variables. The joint cumulative distribution function is given by
$$\HLTW{F_{_{X,Y}}(\HLTY{x},\HLTY{y})}:= P\left(X\leq \HLTY{x}, Y\leq \HLTY{y} \right) .$$
}

$$\qBrd[3.0in]{teal!40}{ \Large  Joint CDF from Joint p.d.f.   }$$
\qBrd[4.6in]{olive!40}{
If  the joint probability density function of X and Y is  $f_{_{X,Y}}(x, y).$ then 
$$ \HLTW{F_{_{X,Y}}(\HLTY{x},\HLTY{y})}= \HLTEQ[teal!30]{\displaystyle \mathop{\int\int}_{\left\{ \Col{{\HLTW{s}\leq \HLTY{x}, \HLTW{t} \leq \HLTY{y} },{\text{ where } (s,t)\in \HLTW{\support[ X,Y]}}}\right\}} f_{_{X,Y}}(\HLTW{s}, \HLTW{t}) ds\; dt }$$
}
\end{frame}



\begin{frame}
\sqBullet{babyblue} If CDF of a bivariate continuous random variable is provided, then the corresponding p.d.f is obtained by following: 
\qBrd[4in]{babyblue!40}{$$\HLTW{\displaystyle f_{_{X,Y}}(x, y)  = \frac{d^2F(x,y)}{dx\;dy}}
$$}
\vspace{.1in}\\

%\qBrd[4in]{teal!40}{
%$$F(x, y)  =  \iint\limits_{\{(s\leq x, t\leq y): (\HLTY{s},\HLTW{t})\in \support[XY]\}}  f_{_{X,Y}}(\HLTY{s}, \HLTW{t})ds \;dt$$}
\end{frame}





\begin{frame}\frametitle{Marginal  Distributions for Continuous Random Vector}
\qbx[4.2in]{babyblue!40}{
The marginal probability mass function of $X$ is given by
\begin{center}
\qBrd[3.2in]{amethyst!40}{
$$f_{_X}(\HLTY{x})=  \int\limits_{\{t: (\HLTY{x},\HLTW{t})\in \support[XY]\}}  f_{_{X,Y}}(\HLTY{x}, \HLTW{t})dt $$
}
\end{center}
}
\vspace{.1in}

\qbx[4.2in]{amethyst!50}{
The marginal probability mass function of $X$ is given by
\begin{center}
\qBrd[3.2in]{babyblue!40}{
$$f_{_Y}(\HLTY{y})=  \int\limits_{\{s: (\HLTY{s},\HLTW{y})\in \support[XY]\}}  f_{_{X,Y}}(\HLTW{s}, \HLTY{y})ds $$}
\end{center}
}
\vspace{.1in}

\end{frame}






\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
\Exmpl{amethyst}{}  The joint pdf of X; Y is given by 
$$f_{_{X,Y}}(x,y)=
 \begin{cases}
\frac{x+y+1}{2} &  \text{ for }  0 < x <1,0 < y <1\\
0 &  \text{otherwise}\end{cases}$$
\begin{enumerate}[a).]
\item Find the cumulative distribution function of $(X, Y)$ .
\item  Find the marginal density of X.
\item  Find the marginal density of Y .
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}



\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{\tiny
\Exmpl{amethyst}{}  The joint pdf of X; Y is given by 
$$f_{_{X,Y}}(x,y)=
 \begin{cases}
\frac{x+y+1}{2} &  \text{ for }  0 < x <1,0 < y <1\\
0 &  \text{otherwise}\end{cases}$$
\vspace{-.2in}
\begin{enumerate}[a).]
\item Find the cumulative distribution function of $(X, Y)$ .
\item  Find the marginal density of X.
\item  Find the marginal density of Y .
\end{enumerate}
}\\
%\pause
\qBrd[4.6in]{babyblue!40}{\tiny
$\displaystyle F_{_{X,Y}}(x,y):= \iint\limits_{\{(s,t)\in \support[XY]: s\leq x, t\leq y\} } f_{_{X,Y}}(s,t)dtds =\int\limits_{0}^s\int\limits_{0}^t\frac{s+t+1}{2}dtds = \frac{xy(x+y+2)}{4}$ for $0 < x < 1, 0 < y < 1$. }
\qBrd[4.6in]{blue!40}{\tiny
$\displaystyle f_{_{X}}(x):= \int\limits_{\{x: (x,y)\in \support[XY]\}} f_{_{X,Y}}(x,y)dy =\int\limits_{0}^1\frac{x+y+1}{2}dy = \frac{x}{2}+ \frac{3}{4}$ for $0 < x < 1$. }

\qBrd[4.6in]{babyblueeyes!20}{\tiny
$\displaystyle f_{_{Y}}(y):= \int\limits_{\{x: (x,y)\in \support[XY] \}} f_{_{X,Y}}(x,y)dx =\int\limits_{0}^1\frac{x+y+1}{2}dx = \frac{y}{2}+ \frac{3}{4}$ for $0 < y < 1$. }

\end{frame}






\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amber!40}{
\Exmpl{amber}{}  Let X, Y have joint cdf
$$F_{_{X,Y}}(x,y)=
 \begin{cases}
x^2y^3&  \text{ for }  0 < x <1,0 < y <1\\
 &  \text{otherwise}\end{cases}$$
 \vspace{-.2in}
\begin{enumerate}[a).]
\item Find the joint density function of $(X, Y)$ .
\item  Find the marginal density of X.
\item  Find the marginal density of Y .
\end{enumerate}
}\\
\vspace{1.7in}
\end{frame}





\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amber!40}{\tiny
\Exmpl{amber}{}  Let X, Y have joint cdf
$$F_{_{X,Y}}(x,y)=
 \begin{cases}
x^2y^3&  \text{ for }  0 < x <1,0 < y <1\\
 &  \text{otherwise}\end{cases}$$
\begin{enumerate}[a).]
\item Find the joint density function of $(X, Y)$ .
\item  Find the marginal density of X.
\item  Find the marginal density of Y .
\end{enumerate}
}\\
%\pause
{\tiny {:}  
%\pause
{\tiny {:}  
\qBrd[4.6in]{babyblue!40}{\tiny
$\displaystyle f_{_{X,Y}}(x,y)= \frac{d^2F_{_{X,Y}}(x,y)}{dx\;dy} =6xy^2 $ for $0\leq x\leq 1, 0\leq y\leq 1$. }
\qBrd[4.6in]{blue!40}{\tiny
$\displaystyle f_{_{X}}(x):= \int\limits_{\{y : (x,y)\in \support[XY] \}} f_{_{X,Y}}(x,y)dy =\int\limits_{0}^1 6xy^2dy =2x$ for $0 \leq  x \leq  1$. }

\qBrd[4.6in]{babyblueeyes!20}{\tiny
$\displaystyle f_{_{Y}}(y):= \int\limits_{\{x: (x,y)\in \support[XY] \}} f_{_{X,Y}}(x,y)dx =\int\limits_{0}^1  6 xy^2 dx =3y^2$ for $0 \leq y \leq  1$. }
}
}
\end{frame}






\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{babyblue!40}{
\Exmpl{babyblue}{}  The joint density of X and Y is given by 
$$f_{_{X,Y}}(x,y)=
 \begin{cases}
2e^{-x-2y}&  \text{ for }  0 < x <\infty,0 < y <\infty\\
 &  \text{otherwise}\end{cases}$$
\begin{enumerate}[a).]
\item  Find the marginal density of X.
\item  Find the marginal density of Y .
\item Find $P(X > 1,  Y < 1) $
\item Find $P(X <Y) $
\item Find $P(X <4) $
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}





\begin{frame}\frametitle{}

\vspace{-.2in}
\qbx[4.5in]{babyblue!40}{\tiny 
\Exmpl{babyblue}{}  The joint density of X and Y is given by 
$ \displaystyle f_{_{X,Y}}(x,y)=
 \begin{cases}
2e^{-x-2y}&  \text{ for }  0 < x <\infty,0 < y <\infty\\
 &  \text{otherwise}\end{cases}$
 \vspace{-.1in}
\begin{enumerate}[a).]
\item  Find the marginal density of X.
\item  Find the marginal density of Y .
\item Find $P(X > 1,  Y < 1) $
\item Find $P(X <Y) $
\item Find $P(X <4) $
\end{enumerate}
}\\
%\pause
{\tiny 
\qBrd[4.6in]{blue!40}{\tiny
$\displaystyle f_{_{X}}(x):= \int\limits_{\{x: (x,y)\in \support[XY] \}} f_{_{X,Y}}(x,y)dy =\int\limits_{0}^{\infty} 2e^{-x-2y} dy =e^{-x}$ for $0 \leq  x <\infty$. }

\qBrd[4.6in]{babyblueeyes!20}{\tiny
$\displaystyle f_{_{Y}}(y):= \int\limits_{\{y: (x,y)\in \support[XY] \}} f_{_{X,Y}}(x,y)dx =\int\limits_{0}^{\infty} 2e^{-x-2y} dx =2e^{-2y}$ for $0 \leq y \leq  \infty$. }
}

\qBrd[4.6in]{babyblueeyes!20}{\tiny
$\displaystyle P(X > 1,  Y < 1) =  \iint\limits_{\{ (x,y)\in \support[XY] : x>1, y<1\}} f_{_{X,Y}}(x,y)dx =\int\limits_{1}^{\infty}  \int\limits_{0}^{1} 2e^{-x-2y} dx =e^{-1}- e^{-3}$. }
\qBrd[4.6in]{olive!30}{\tiny
$\displaystyle P(X<Y) =  \iint\limits_{\{ (x,y)\in \support[XY] : x<y\}} f_{_{X,Y}}(x,y)dx =\int\limits_{0}^{\infty}  \int\limits_{x}^{\infty} 2e^{-x-2y} dy dx =\frac{1}{3}$. }
\qBrd[4.6in]{teal!30}{\tiny
$\displaystyle P(X<4) =  \iint\limits_{\{ x\in \support[X] : x<4\}} f_{_{X}}(x)dx =\int\limits_{0}^{4} e^{-x} dx =1-e^{-4}$. }

\end{frame}









\section{Conditional Distributions}
\TransitionFrame[bittersweet]{\Large Conditional Distributions  }



\begin{frame}
\define{Conditional p.m.f}{
If $\pmf_{_{XY}}(x, y)$ denotes the joint probability mass function (pmf) of two discrete random variables X and Y and if $\pmf_X(x)$ and $\pmf_Y (y)$ denote the marginal probability function of $X,$ ($Y $ respectively) then the conditional probability of X given $Y = y $ is given by $$\pmf_{_{X\mid Y}}(x\mid y)=  \frac{\pmf_{_{X,Y}}(x,y)}{\pmf_{_Y}(y)}$$
The conditional probability of Y given X = x is given by
$$\pmf_{_{ Y\mid X}}(y \mid x)=  \frac{\pmf_{_{X,Y}}(x,y)}{\pmf_{_X}(x)}.$$
}
\end{frame}





\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
\Exmpl{amethyst}{} Suppose two cards are drawn at random without replacement from a deck of 4 cards numbered 1, 2, 3, 4. Let X be the number on the first card and Y be the number of the second card.
\begin{enumerate}[a).]
\item Find the conditional probability of X given Y = 2. 
\item  Use this to compute $P(X\leq 2\mid Y = 2).$
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}





\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
\Exmpl{amethyst}{} \tiny Suppose two cards are drawn at random without replacement from a deck of 4 cards numbered 1, 2, 3, 4. Let X be the number on the first card and Y be the number of the second card.
\begin{enumerate}[a).]
\item Find the conditional probability of X given Y = 2. 
\item  Use this to compute $P(X\leq 2\mid Y = 2).$
\end{enumerate}
}\\
%\pause
\vspace{.1in}
\qBrd[4.6in]{teal!30}{\tiny
The conditional distribution of X given Y=2 is calculated for each possible value of X using $$\HLTW{ \displaystyle \pmf_{_{X\mid Y=2}}(x)= \frac{\pmf_{_{X, Y}}(x, 2)}{ \pmf_{_{Y}}(2)}.}$$ The table below shows the results\\
\begin{tabular}{|l|l|}
\hline
x &   $\pmf_{_{X\mid Y=2}}(x)$  \\ \hline
1 & $\frac{1}{3}$ \\ \hline
2 & 0   \\ \hline
3 &$\frac{1}{3}$ \\ \hline
4 & $\frac{1}{3}$ \\ \hline
\end{tabular}
}


 \qBrd[4.6in]{babyblue!40}{\tiny
$P(X\leq 2\mid Y=2)= \pmf_{_{X\mid Y=2}}(1)+  \pmf_{_{X\mid Y=2}}(2)= \frac{1}{3}$.
}


\end{frame}





\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{teal!40}{
\Exmpl{teal}{} Suppose that 3 balls are randomly selected from an urn containing 3 red, 4 white, and 5 blue balls. If we let X and Y denote respectively, the number of red and white balls chosen.
\begin{enumerate}[a).]
\item Find the conditional probability of X given Y = 2.
\item  Use this to compute $P(X\leq 2\mid Y = 2).$
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}





\begin{frame}
\define{Conditional p.d.f. }{
If $f_{_{XY}}(x, y)$ denotes the joint probability density function of two continuous random variables X and Y and if $f_X(x)$ and $f_Y (y)$ denote the marginal probability density function of $X,$ ($Y $ respectively) then the conditional probability density of X given $Y = y $ is given by $$f_{_{X\mid Y}}(x\mid y)=  \frac{f_{_{X,Y}}(x,y)}{f_{_Y}(y)}$$
The conditional probability density of Y given X = x is given by
$$f_{_{ Y\mid X}}(y \mid x)=  \frac{f_{_{X,Y}}(x,y)}{f_{_X}(x)}.$$
}
\end{frame}




\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
\Exmpl{amethyst}{}  The joint pdf of X; Y is given by 
$$f_{_{X,Y}}(x,y)=
 \begin{cases}
\frac{x+y+1}{2} &  \text{ for }  0 < x <1,0 < y <1\\
0 &  \text{otherwise}\end{cases}$$
\begin{enumerate}[a).]
\item Find the conditional probability of X given Y = 0.5.
\item  Use this to compute $P(X\leq 0.75 \mid Y=0.5)$
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}




\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{\tiny 
\Exmpl{amethyst}{}  The joint pdf of X; Y is given by 
$$f_{_{X,Y}}(x,y)=
 \begin{cases}
\frac{x+y+1}{2} &  \text{ for }  0 < x <1,0 < y <1\\
0 &  \text{otherwise}\end{cases}$$
\begin{enumerate}[a).]
\item Find the conditional probability of X given Y = 0.5.
\item  Use this to compute $P(X\leq 0.75 \mid Y=0.5)$
\end{enumerate}
}\\
%\pause
\qBrd[4.6in]{babyblueeyes!20}{\tiny Note that, in an earlier example we have computed the marginal as follows\\
$\displaystyle f_{_{Y}}(y):= \int\limits_{\{x: (x,y)\in \support[XY] \}} f_{_{X,Y}}(x,y)dx =\int\limits_{0}^1\frac{x+y+1}{2}dx = \frac{y}{2}+ \frac{3}{4}$ for $0 < y < 1$. }

\qBrd[4.6in]{olive!30}{a). \tiny The Conditional density of $X$ given $Y= 0.5$ is  
$\displaystyle  f_{_{X\mid Y=0.5}}(x)= \frac{ f_{_{X, Y}}(x, 0.5)}{f_{_Y}(0.5)}= \frac{x}{2}+\frac{3}{4} $ for $0 < x < 1$. }

\qBrd[4.6in]{teal!30}{b). \tiny 
$\displaystyle  P (X\leq 0.75 \mid Y=0.5)= \int\limits_{0}^{0.75} f_{_{X\mid Y=0.5}}(x)dx = \int\limits_{0}^{0.75}  ( \frac{x}{2}+\frac{3}{4})dx= \frac{27}{32}$. }

\end{frame}





\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amber!40}{
\Exmpl{amber}{}  Let X, Y have joint cdf
$$F_{_{X,Y}}(x,y)=
 \begin{cases}
x^2y^3&  \text{ for }  0 < x <1,0 < y <1\\
 &  \text{otherwise}\end{cases}$$
\begin{enumerate}[a).]
\item Find the conditional probability of X given Y = 0:5.
\item Use this to compute $P(X\geq 0.5 \mid Y=0.5)$
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}






\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{babyblue!40}{
\Exmpl{babyblue}{}  The joint density of X and Y is given by 
$$f_{_{X,Y}}(x,y)=
 \begin{cases}
2e^{-x-2y}&  \text{ for }  0 < x <\infty,0 < y <\infty\\
 &  \text{otherwise}\end{cases}$$
\begin{enumerate}[a).]
\item  Find the conditional probability of X given Y = 1.
\item  Find the marginal density of Y .
\item Use this to compute $P(X\leq 2 \mid Y=1)$
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}

	


\section{Statistically Independent Random Variables}
\TransitionFrame[bittersweet]{\Large Statistically Independent Random Variables }



\begin{frame}
\define{Independent Random Variables}{
Two continuous random variables X and Y are said to be independent if the corresponding joint probability density function  $$ \HLTW{ \displaystyle f_{_{X,Y}}(x, y) = f_{_X}(x)  f_{_Y} (y)}$$ for all x and y where $f_{_X}(x)$, and $ f_{_Y} (y)$ are the marginal density for X, and Y respectively. 
}
\end{frame}


\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
\Exmpl{amethyst}{}  The joint pdf of X; Y is given by 
$$\HLTW{f_{_{X,Y}}(x,y)=
 \begin{cases}
\frac{x+y+1}{2} &  \text{ for }  0 < x <1,0 < y <1\\
0 &  \text{otherwise}\end{cases}}$$
\begin{enumerate}[a).]
\item Are X and Y independent?
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
\end{frame}



\begin{frame}\frametitle{}

\vspace{-.1in}
\qbx[4.5in]{amethyst!40}{
\Exmpl{amethyst}{}  The joint pdf of X; Y is given by 
\vspace{-.1in}
$$
\HLTW{f_{_{X,Y}}(x,y)=
 \begin{cases}
\frac{x+y+1}{2} &  \text{ for }  0 < x <1,0 < y <1\\
0 &  \text{otherwise}\end{cases}}$$
\vspace{-.2in}
\begin{enumerate}[a).]
\item Are X and Y independent?
\end{enumerate}
}\\
\vspace{0.1in}
%\pause
%\vspace{1.7in}
\qBrd[4.6in]{blue!40}{\tiny We have already seen in a previous example that the marginals: \\
\qBrd[4.4in]{babyblueeyes!20}{\tiny
$\displaystyle f_{_{X}}(x):= \int\limits_{\{x: (x,y)\in \support[XY]\}} f_{_{X,Y}}(x,y)dy =\int\limits_{0}^1\frac{x+y+1}{2}dy = \frac{x}{2}+ \frac{3}{4}$ for $0 < x < 1$. }\\
\qBrd[4.4in]{babyblueeyes!20}{\tiny
$\displaystyle f_{_{Y}}(y):= \int\limits_{\{x: (x,y)\in \support[XY] \}} f_{_{X,Y}}(x,y)dx =\int\limits_{0}^1\frac{x+y+1}{2}dx = \frac{y}{2}+ \frac{3}{4}$ for $0 < y < 1$. }
}\\

\vspace{0.1in}
\qBrd[4.6in]{olive!30}{\tiny Now observe that 
$\HLTW{f_{_{X}}(x)\times f_{_{Y}}(y) = (\frac{x}{2}+\frac{3}{4}) \times  (\frac{y}{2}+\frac{3}{4}) \HLTY{\neq}  \frac{x+y+1}{2} =f_{_{X, Y}}(x, y)} $
Therefore, the random variables $X$ and $Y$ are {\bf  NOT statistically independent.}  
}


\end{frame}








\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{amber!40}{
\Exmpl{amber}{}  Let X, Y have joint cdf
$$\HLTW{ F_{_{X,Y}}(x,y)=
 \begin{cases}
x^2y^3&  \text{ for }  0 < x <1,0 < y <1\\
 &  \text{otherwise}\end{cases}}$$
\begin{enumerate}[a).]
\item Are X and Y independent?
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}






\begin{frame}\frametitle{}

\vspace{-.1in}
\qbx[4.5in]{babyblue!40}{
\Exmpl{babyblue}{}  The joint density of X and Y is given by 
$$\HLTW{f_{_{X,Y}}(x,y)=
 \begin{cases}
2e^{-x-2y}&  \text{ for }  0 < x <\infty,0 < y <\infty\\
 &  \text{otherwise}\end{cases}}$$
 \vspace{-.2in}
\begin{enumerate}[a).]
\item Are X and Y independent?
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}





\section{Expectation for  Different Functions of Multivariate  Random Variables}
\TransitionFrame[bittersweet]{\Large Expectation for  Different Functions of Multivariate  Random Variables}


\begin{frame}
\qbx[4.4in]{babyblue!40}{
Let X, Y be two discrete random variables with joint probability function $\pmf_{_{X,Y}}(x, y)$. Then the expected value of $g(X, Y )$ is given by
$$\HLTW{ \displaystyle  E\left( g(X,Y) \right)=  \sum_{(x,y)\in \support[XY]} g(x,y) \pmf_{_{X,Y}}(x,y)}$$
}
\vspace{.1in}

\qbx[4.4in]{amethyst!40}{
Let X, Y be two continuous random variables with joint probability density function $f_{_{X,Y}}(x, y)$. Then the expected value of $g(X, Y )$ is given by \vspace{-.25in}
$$\HLTW{ \displaystyle E\left( g(X,Y) \right)=  \int\limits_{(x,y)\in \support[XY]} g(x,y) f_{_{X,Y}}(x,y) dx\; dy} $$
}
\end{frame}



\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{olive!40}{
\Exmpl{olive}{}  Let X, Y have joint cdf
$$ \HLTW{ f_{_{X,Y}}(x,y)=
 \begin{cases}
\frac{2}{7}(x+2y)&  \text{ for }  0 < x <1,1 < y <2\\
 &  \text{otherwise}\end{cases}}$$
 \vspace{-.2in}
\begin{enumerate}[a).]
\item Find the expected value of $\frac{X}{Y^3}$
\item Find the expected value of $XY$
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}


\section{Variance and Covariance of a Random Variable }
\begin{frame}\frametitle{Reminder: Mean and Variance of a Random Variable}
\qbx[4.6in ]{amethyst!60}{
\qBrd[4.5in ]{amber!40}{\qBrd[.8in ]{amber!60}{Mean:} Let $X$ be a random variable, then  $E(X)$ denoted by $\mu_{_X}$ is called the {\bf mean} of the random variable. }\\
\qBrd[4.5in ]{olive!40}{ \qBrd[.8in ]{olive!60}{Variance:} Let $X$ be a random variable, then  $E\left(X-\mu_{_X}\right)^2$ denoted by Var${(X)}$ is called the {\bf Variance} of the random variable.  Note that, the alternative formula for variance is: 
 $$ \text{Var}(X):= E(X^2)- \left(E(X)\right)^2.$$
}
}


\end{frame}


\begin{frame}\frametitle{Covariance}
\begin{center}
\define{Covariance}{
Let $X$, and $Y$ be two random variables with a joint distribution.  Then $$\HLTW{\text{Cov}(X,Y)=E \left( (X-\mu_{_X}) (Y-\mu_{_Y})\right),} $$
where $\mu_{_X}$ and $\mu_{_Y}$ denotes the mean of the random variables $X$, and $Y$ respectively. \\
\vspace{.1in}
}
\vspace{-.15in}
\qBrd[4.2in ]{amethyst!40}{
An Alternative Formulation for the covariance is the following:\\
$$\qBrd[2.5in ]{amethyst!60}{{ \HLTW{\text{Cov}(X,Y)}= \HLTW{E(XY)- E(X)E(Y)} }}$$
\vspace{-.1in}
}
\end{center}

\end{frame}

\begin{frame}\frametitle{Statistically Independent Random Variables and Covariance}

\begin{theorem}
If $X$, and $Y$ are two {\bf statistically independent} random variables, then 
$$\HLTW{\text{Cov}(X,Y)=0 }.$$
However, the converse of the result is not true in general. 
\end{theorem}
\end{frame}










\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{brightpink!40}{
\Exmpl{brightpink}{}  Suppose X and Y have the following joint
distribution
{\tiny {:}  
\begin{table}[!ht]
\centering
\begin{tabular}{ |c|| c| c| c| } 
\toprule
\diagbox{Y}{X} & \makecell{ 0}& \makecell{1} & \makecell{2}   \\ 
\midrule
\midrule
0 &   $\frac{1}{6}$ &  $\frac{1}{3}$ &  $\frac{1}{2}$\\\hline
1&   $\frac{2}{9}$& $\frac{1}{6}$& 0 \\\hline
2 &   $\frac{1}{36}$ &  0 & 0    \\\hline
\bottomrule
\end{tabular}
\end{table}
\begin{enumerate}
\item Find the covariance of X and Y .
\item Show that  X,  and Y are not  statistically independent?
\end{enumerate}
}
}\\
%\pause
\vspace{1.7in}

\end{frame}






\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{olive!40}{
\Exmpl{olive}{}  Let X and Y have joint density
$$\HLTW{f_{_{X,Y}}(x,y)=
 \begin{cases}
2&  \text{ for }  x>0, y>0, x+y<1\\
 &  \text{otherwise}\end{cases}}$$
\begin{enumerate}[a).]
\item Find the covariance of X and Y .
\item Are the random variables X,  and Y statistically independent?
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}


\TransitionFrame[antiquefuchsia]{\Large Expected Value of Linear Combination  }

\begin{frame}


\begin{center}
\qbx[4.6in ]{amethyst!40}{
Let $X_1, X_2, \ldots X_n$ are random variables and $\HLTW{\displaystyle  Y =a_0+ \sum_{i=1}^{n} a_i X_i }$, where $a_i's$ are constants then 
\begin{enumerate}
 \qBrd[3.1in ]{amber!40}{ \item[\sqBullet{amber}] $\displaystyle E(Y)=  a_0+\sum_{i=1}^{n} a_i E\left(X_i\right) $}
 \qBrd[4.1in ]{applegreen!40}{  \item[\sqBullet{olive}] $\displaystyle \text{Var}(Y)=  \sum_{i=1}^{n} a^2_i \text{Var}\left(X_i\right)+ 2 \mathop{\sum\sum}_{1\leq i <j \leq n}  a_i a_j \text{Cov}(X_i, X_j) $}
\end{enumerate}
}
\vspace{.1in}
 \qBrd[4.1in ]{green!40}{If $X_1, X_2, \ldots X_n$are 
 mutually statistically independent then, 
 $$\HLTW{\text{Var}(Y)=  \sum_{i=1}^{n} a^2_i \text{Var}\left(X_i\right)}$$}
 \end{center}
\end{frame}



\begin{frame}\frametitle{}
\begin{center}
\qbx[4.6in ]{amethyst!40}{
Let $X_1, X_2, \ldots X_n$ are random variables. $$\HLTW{\displaystyle Y_1 =a_0+ \sum_{i=1}^{n} a_i X_i },  \text{ and }  \HLTW{\displaystyle Y_2 =b_0+ \sum_{i=1}^{n} b_i X_i },$$ \vspace{-.1in} \\
where $a_i's$ abd $b_i's$ are constants then \\
\qBrd[4.2in ]{applegreen!40}{$\HLTW{\displaystyle \text{Cov}(Y_1, Y_2)=  \sum_{i=1}^{n} a_i b_i \text{Var}\left(X_i\right)+ 2\mathop{\sum\sum}_{1\leq i <j \leq n}  a_i b_j \text{Cov}(X_i, X_j)} $}
}

 \qBrd[4.1in ]{green!40}{If $X_1, X_2, \ldots X_n$are 
 mutually statistically independent then, 
 $$\HLTW{ \displaystyle \text{Cov}(Y)=  \sum_{i=1}^{n} a_i b_j \text{Var}\left(X_i\right)}$$}
 \end{center}
\end{frame}



\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{olive!40}{
\Exmpl{olive}{}Let X and Y have joint distribution.  For X and Y defined in the previous two examples,  Let $Z_1= 2X+4Y$ and $Z_2= X-2Y$
\begin{enumerate}[a).]
\item Find  $E(Z_1)$, $E(Z_2)$
\item Find Var$(Z_1)$, Var$(Z_2)$
\item Find Cov$(Z_1,Z_2)$.
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}






\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{teal!40}{
\Exmpl{teal!60}{}Let X and Y be two independent random variables with means $2$,  $3$ respectively. , The variances of $X,Y$ is provided as $4$ and  $2$. Let $Z_1=X+2Y+3$ and $Z_2=3X-Y$.  Find: 
\begin{enumerate}[a).]
\item Find  $E(Z_1)$, $E(Z_2)$
\item Find Var$(Z_1)$, Var$(Z_2)$
\item Find Cov$(Z_1,Z_2)$.
\end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}



\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{babyblue!40}{\tiny
\Exmpl{babyblue}{}  Gasoline is to be stocked in a bulk tank once at the beginning of each week and then sold to individual customers. Let $Y_1$ denote the proportion of the capacity of the bulk
tank that is available after the tank is stocked at the beginning of the week. Because of the limited supplies, $Y_1$ varies from week to week. Let $Y_2$ denote the proportion of the capacity of the bulk tank that is sold during the week. Because $Y_1$ and $Y_2$ are both proportions, both variables take on values between 0 and 1. Further, the amount sold, $Y_2$, cannot exceed the amount available, $Y_1$.  Suppose that the joint density function for $Y_1$ and $Y_2$ is given by
$$f_{_{Y_1,Y_2}}(y_1,y_2)=
 \begin{cases}
3y_1&  \text{ for }  0 < x <\infty,0 \leq  y_2\leq y_1 \leq 1\\
 &  \text{otherwise}\end{cases}$$
 \begin{enumerate}[a).]
 \item  Find the probability that less than one-half of the tank will be stocked and more than one-quarter of the tank will be sold. i.e. $P(0\leq  Y_1 \leq 0.5, Y_2 > 0.25).$
 \item What is the probability that  less than one-half of the tank will be stocked given that more than one-quarter of the tank will be sold.  
 $P(0\leq  Y_1 \leq 0.5\mid Y_2 > 0.25).$
 \item Find the marginal density of $Y_1$
 \item Find the marginal density of $Y_2$
 \item Find $E(Y_2)$
 \item Find the conditional density of $Y_2$ given $Y_1=0.25$.
 \end{enumerate}
 % Ans: 5/128
 \vspace{-.1in}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}



\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{olive!30}{\tiny
\Exmpl{olive}{}  Given here is the joint probability function associated with data obtained in a study of automobile
accidents in which a child (under age 5 years) was in the car and at least one fatality occurred. Specifically, the study focused on whether or not the child survived and what type of seatbelt (if any) he or she used. Define
\begin{center}
$$Y_1= 
 \begin{cases}
0&  \text{ if the child survived }\\
 1&  \text{if not,}\end{cases} \text{ and,  }  
 Y_2= 
 \begin{cases}
0&  \text{ if no belt used, }\\
 1&  \text{ if adult belt used}\\
 2&  \text{ if car-seat belt used}
 \end{cases} $$
 Notice that $Y_1$ is the number of fatalities per child and, since children's car seats usually utilize two belts, $Y_2$ is the number of seatbelts in use at the time of the accident	\\
\begin{tabular}{ |c|| c| c| } 
\toprule
\diagbox{$y_2$}{$y_1$} & \makecell{ 0}& \makecell{1}   \\ \hline
0 & 0.38&  0.17 \\\hline
1 &  0.14  & 0.02 \\\hline
2&0.24  &0.05 \\\hline
\bottomrule
\end{tabular}
\begin{enumerate}
\item Find F(1, 2). What is the interpretation of this value?
\item What is the Marginal distribution of $Y_1$?
\item What is the Marginal distribution of $Y_2$?
\end{enumerate}
\end{center}
 % Ans: 5/128
% \vspace{-.1in}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}







\begin{frame}\frametitle{Example}

\vspace{-.1in}
\qbx[4.5in]{applegreen!40}{\tiny
\Exmpl{applegreen}{}  The management at a fast-food outlet is interested in the joint behavior of the random variables $Y_1$, defined as the total time between a customer's arrival at the store and departure from the service window, and $Y_2$, the time a customer waits in line before reaching the service window.
Because $Y_1$ includes the time a customer waits in line, we must have $Y_1 \geq  Y_2$.  The relative frequency distribution of observed values of $Y_1$ and $Y_2$ can be modeled by the probability density function
$$f_{_{Y_1,Y_2}}(y_1,y_2)=
 \begin{cases}
e^{-y_1}&  \text{ for }  0 \leq  y_2\leq y_1 \leq \infty\\
 0&  \text{otherwise}\end{cases},$$ 
 with time measured in minutes. Find
 % Ans: 5/128
% \vspace{-.1in}
 \begin{enumerate}[a).]
 \item $P(Y_1<2, Y_2>1).$
 \item $P(Y_1 \geq  2Y_2).$
 \item $P(Y_1-Y_2 \geq  1).$
 \item Find $E(Y_1-Y_2)$, expected service time. 
 \item Find Marginal Density of $Y_1$.
  \item Find Marginal Density of $Y_2$.
    \item Find Conditional  Density of $Y_2$ given $Y_1=2$.
    \item Find $E(Y_2\mid Y_1=2)$
 \end{enumerate}
}\\
%\pause
\vspace{1.7in}
{\tiny {:}  

}
\end{frame}







\section{Moment Generating Function}
\TransitionFrame[antiquefuchsia]{\Large Moment Generating Function }

\begin{frame}
\vspace{-.1in}
\define{Moment Generating Function}{
The {\bf moment generating function} of a random variable X is given by \vspace{-.1in} $$M_{X}(t)= E\left( e^{tX}\right) $$
\vspace{-.2in}
}

\qBrd[4.6in]{amber!40}{ \sqBullet{amber}  If $X$ is a discrete random variable with a probability mass funciton (pmf) $\pmf_{_X}(x)$ on the support of the random variable $\support[X]$, then assuming the existance/finiteness of the quantity
\vspace{-.2in}
$$\HLTW{\displaystyle M_{X}(t):=E\left( e^{tX}\right)= \sum_{x\in \support[X]} e^{tx} \pmf_{_X}(x).}$$
\vspace{-.1in}
}
\qBrd[4.6in]{applegreen!40}{ \sqBullet{applegreen} If $X$ is a continuous random variable with a probability density funciton (pdf) $f_{_X}(x)$ on the support of the random variable $\support[X]$, then assuming the existance/finiteness of the quantity
\vspace{-.15in}
$$\HLTW{ \displaystyle M_{X}(t):=E\left( e^{tX}\right)= \int\limits_{x\in \support[X]} e^{tx} \pmf_{_X}(x)dx. \vspace{-.2in}}$$
}
\end{frame}


\begin{frame}
\define{Raw Moments of a Random Variable}{
Let $r$ be a positive integer, then  the  $r^{\text{th}}$ raw moments (non-centered) of a random variable X is defined as  $\mu_{_{r:X}}'= E(X^{r})$.
}
\begin{theorem}
Let X be a r.v.  with the moment generating function $M_{_X}(t)$, then, assuming existence,  the $r^{\text{th}}$ raw moments (non-centered) for the random variable can be obtained as 
$$ \HLTW{\mu_{_{{r}:X}}'= \frac{d^{{r}} M_{_X}(t) }{dt^{{r}}}\Big\vert_{t=0} } $$
\end{theorem}
\end{frame}


\begin{frame}\frametitle{Discuss Uniqueness of MGF}

\end{frame}


\begin{frame}\frametitle{Example}

\end{frame}

\TransitionFrame[antiquefuchsia]{\Large Questions?  }
 
 
\end{document}
